\documentclass[11pt]{article}
\usepackage[french]{babel}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}  % \includegraphics[scale=0.6]{image.png}\\
\usepackage{color}  % \textcolor{declared-color}{text}
\usepackage{mdframed}  % Cadres de couleurs \begin{mdframed}[linecolor=...] \end{mdframed}
\usepackage{amsfonts}  % Pour écrire les ensembles IR, IK etc "\mathbb  "
\usepackage{amsmath}  % Utilisation de \underset
\usepackage{amssymb}  % Utilisation de \underset
\usepackage{hyperref}  % Mettre le lien vers la page de google drive
\usepackage{times}
\usepackage{fullpage}
\usepackage{tikz} % Pour Sarrus
\usetikzlibrary{babel}
\usetikzlibrary{calc,matrix}
\usetikzlibrary{calc,tikzmark}

\newcommand{\MyTikzmark}[2]{%
    \tikz[remember picture,baseline]
        \node [anchor=base, inner sep=0pt, outer sep=0pt]
            {\tikzmark{#1 LEFT}$#2$\tikzmark{#1 RIGHT}};%
 }

\newcommand{\DrawVLine}[3][]{%
  \begin{tikzpicture}[overlay,remember picture]
    \draw[shorten <=-1.7ex, shorten >=-0.3ex, #1]
            ($(pic cs:#2 LEFT)!0.5!(pic cs:#2 RIGHT)$) --
            ($(pic cs:#3 LEFT)!0.5!(pic cs:#3 RIGHT)$);
  \end{tikzpicture}
}

\newcommand{\DrawHLine}[3][]{%
  \begin{tikzpicture}[overlay,remember picture]
    \draw[shorten <=-0.2em, shorten >=-0.3em, yshift=0.7ex, #1]
            (pic cs:#2 LEFT) --  (pic cs:#3 RIGHT);
  \end{tikzpicture}
}

\renewcommand{\parallel}{\mathbin{/\negthickspace/}}

\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\ImAppli}{Im}
\DeclareMathOperator{\Vect}{Vect}
\DeclareMathOperator{\R}{\mathbb R}
\DeclareMathOperator{\Mat}{Mat}
\DeclareMathOperator{\MatK}{\Mat_{\mathbb K}}
\DeclareMathOperator{\sign}{sign}

\title{Résumé cours de Maths - Module SI\\ \textbf{Algèbre Linéaire}}
\date{version du 25 mai (21h30)}

\pagestyle{plain}

\begin{document}
    \pagenumbering{Roman}
    \maketitle
    \tableofcontents
    \pagebreak
    \textbf{Contacts: \\Henri Anciaux} \\
    Mail : \textcolor{blue}{henri.anciaux@gmail.com} \\ % Les liens hyper-textes se font automatiquement
    Site : \textcolor{blue}{http://homepages.ulb.ac.be/hanciaux/MATHF112.html} \\
    Bureau : P.O.7.110

    Lien de partage (Google Drive) des scans du cours théorique :\\
    \urlstyle{same}
    \textcolor{blue}{\href{https://drive.google.com/open?id=0B11b8daV9oclfllIMjFsNktRblMzdFdXM3cxazVtUGpqYlY2VHFvaV8tQWlRZEM4VDc2MG8&authuser=0} {LIEN - Cours théorique}}

    \clearpage
    \setcounter{page}{1}

    \pagenumbering{arabic}
    \pagebreak

    \section{Introduction à l'algèbre linéaire}
        On retrouve l'algèbre linéaire dans des domaines aussi nombreux que varié dans les mathématiques.
        Et elle nous permet de pouvoir \textbf{simplifier les choses}.

        \includegraphics[scale=0.6]{figures/graphiques-fonctions.png}

        $f(x+h) \approx f(x)+h \times f'(x)$ (linéaire)

        Par exemple le Polynôme de Taylor (qui est un cas d'algèbre linéaire)
        nous permet de simplifier des expressions mathématiques en les linéarisant
        et donc de pouvoir résoudre certains calculs insolubles sans. Comme la
        formule d'oscillation du pendule: $y'' + \sin(y) = 0$ qui est une équation
        différentielle du second ordre impossible à résoudre, peut être simplifiée en $y''+y=0$.
        Elle est aussi utile dans les nuages de points, pour calculer une
        matrice puissance $n$, approximer des fonctions, etc...

    \section{Espaces vectoriels}
        \begin{mdframed}[linecolor=magenta]
            \textcolor{magenta}{\textbf{Notation :}}

            $\mathbb K = \R, \mathbb C$, $\{0, 1\}$ ou n'importe quel "corps".
        \end{mdframed}

        \begin{mdframed}[linecolor=blue]
            \textcolor{blue}{\textbf{Définition :}}

            Un \textbf{espace vectoriel} sur $\mathbb K$ ( = "$\mathbb K$-espace vectoriel")
            est un ensemble E muni de 2 opérations:
            \begin{itemize}
                \item \textbf{addition interne} : $+ : E \times E \to E$
                telle que, $\forall \, u, v, w \in E$ :
                \begin{enumerate}
                    \item + est \textbf{commutative} $\Leftrightarrow v + w = w + v$
                    \item + est \textbf{associative} $\Leftrightarrow u + (v + w) = (u + v) + w$
                    \item + admet un \textbf{élément neutre} noté $\overrightarrow{0_E}$ ou $0 \Leftrightarrow 0 + v = v$
                    \item + admet un \textbf{opposé} noté $-v \Leftrightarrow v + (-v) = 0$
                \end{enumerate}

              \item \textbf{Multiplication externe} : $\cdot: \mathbb K\times E \to E$
              telle que, $\forall \lambda \, \in \mathbb K, v, w \, \in E$:
                \begin{enumerate}
                    \item $\cdot$ est \textbf{distributive à gauche}
                          $\Leftrightarrow \lambda \cdot (v + w) = \lambda \cdot v + \lambda \cdot w$
                    \item $\cdot$ est \textbf{distributive à droite}
                          $\Leftrightarrow (v + w) \cdot \lambda = \lambda \cdot v + \lambda \cdot w$
                    \item $\cdot$ est \textbf{associative}
                          $\Leftrightarrow \lambda \cdot (\mu \cdot v) = (\lambda \cdot \mu) \cdot v$
                    \item $\cdot$ admet un \textbf{élément neutre}
                          $\Leftrightarrow 1 \cdot v = v$
               \end{enumerate}
            \end{itemize}
            Les éléments de E sont appelés vecteurs. Les 2 opérations + et $\cdot$
            constituent une structure vectorielle.
        \end{mdframed}

        \begin{mdframed}[linecolor=cyan]
        \textcolor{cyan}{\textbf{Remarque :}}

            Si E est un e.v. (= Espace Vectoriel),

            \[
                \left \{
                    \begin{aligned}
                        0 \cdot v &= \overrightarrow{0_E} \\
                        (-1) \cdot v &= (-v)
                    \end{aligned}
                \right.
            \]
        \end{mdframed}

        \begin{mdframed}[linecolor=magenta]
            \textcolor{magenta}{\textbf{Notation :}}

            On appelle \textbf{scalaire} tout élément de $\mathbb K$
        \end{mdframed}

        \begin{mdframed}[linecolor=red]
            \textcolor{red}{\textbf{Théorème :}}

             $\mathbb K^n$ est un $\mathbb K$-e.v. (par exemple $\R^n$ ou $\mathbb C^n$)
        \end{mdframed}

        \bigskip

        3 concepts fondamentaux :

        \begin{enumerate}
            \item \textbf{Applications linéaires}
            \item \textbf{Sous-espaces vectoriels}
            \item \textbf{Base et dimensions}
        \end{enumerate}

        \subsection{Sous-espace vectoriel}
            \begin{mdframed}[linecolor=blue]
                \textcolor{blue}{\textbf{Définition :}}

                Soit $E$ un $\mathbb K$-e.v. et $F \subset de$, un sous-ensemble de E.
                On dit que $F$ est un \textbf{sous-espace vectoriel} de E si et seulement si :

                \[
                    \left \{
                        \begin{aligned}
                            v + w \in F \qquad &\forall v, w \in F, v + w \in F \\
                            \lambda \cdot v \in F \qquad &\forall \lambda \in \mathbb K, v \in F
                        \end{aligned}
                    \right.
                \]
            \end{mdframed}

            \begin{mdframed}[linecolor=red]
                \textcolor{red}{\textbf{Théorème :}}

                Soit $\{v\}$ où $v \neq 0_E$ n'est \textbf{pas} un e.v : $v + v = 2v \neq v$.
                Alors : $F = \{\lambda \cdot v \quad t.q. \; \lambda \in \mathbb K \}$ est sous-e.v. de E.
                On l'appelle également \textit{droite vectorielle de $E$ engendrée par $v$}.
            \end{mdframed}

            \begin{mdframed}[linecolor=magenta]
                \textcolor{magenta}{\textbf{Notation :}}

                On note la droite vectorielle $\{\lambda \cdot v \quad t.q. \; \lambda \in \mathbb K \}$
                de la manière suivante : $v \mathbb K$.
            \end{mdframed}

            \begin{mdframed}[linecolor=green]
                \textcolor{green}{\textbf{Exemple :}}

                Pour $E = \R^2$, les sous-ensembles $F_i$ ci dessous
                sont-ils des sous-espaces vectoriels de $E$ ?
                \begin{itemize}
                    \item $F_1 = \{(x, y) \in \R^2 \quad t.q. \; 3x+2y=0\}$
                    \item $F_2 = \{(x, y) \in \R^2 \quad t.q. \; x^2+y^2=1\}$
                    \item $F_2 = \{(x, y) \in \R^2 \quad t.q. \; 3x+2y=1\}$
                \end{itemize}

                Sur un graphique:

                \includegraphics[scale=0.5]{figures/fonctions.png}

                1) $F_1$ est-il un sous-e.v. ?

                \begin{itemize}
                    \item soit $(x, y)$ et $(x', y')$ dans $F_1$, est-ce que
                          $(x, y) + (x', y') \in F_1$ ? $(x, y) + (x', y') = 3(x + x') + 2(y+y') = 3x + 3x' + 2y + 2y' = (3x + 2y) + (3x' + 2y') = 0 + 0 = 0 \in F_1$
                          étant donné que $3x + 2y = 0$ est notre "équation" de départ (voir énoncé).
                          La première propriété pour que $F_1$ soit un sous-e.v. de $E$ est donc vérifiée ;
                    \item soit $(x, y) \in F_1$ et $\lambda \in \R$, est-ce que
                          $\lambda(x, y) \in F_1$? $\lambda(x, y) = 3(\lambda x) + 2(\lambda y) = \lambda(3x + 2y) = \lambda 0 = 0 \in F_1$.
                          La seconde propriété est vérifiée également, donc $F_1$ est un sous-e.v. de $E$.
                          Nous pouvons aussi dire que $3x + 2y = 0$ est une droite vectorielle de E.
                \end{itemize}

                2) Prenons les points $(1, 0)$ et $(0, 1) \in F_2$.
                $(1, 0) + (0, 1) = (1, 1) \notin F_2$, car $1^2 + 1^2 = 2 \neq 1$.
                Donc grâce à un contre-exemple, nous pouvons affirmer que $F_2$ n'est pas un sous-e.v de $E$.

                3) Idem que pour le 2) avec les points $(1, -1)$ et $(3, -4)$
            \end{mdframed}

            \begin{mdframed}[linecolor=cyan]
                \textcolor{cyan}{\textbf{Remarque :}}

                Si $F$ est un sous-ensemble de $E$, alors $0_E \in F$.
                Autrement dit, si le sous-ensemble $F$ ne contient pas l'élément neutre
                de l'ensemble $E$ auquel il appartient, il n'est pas un sous e.v de cet ensemble.
                (Cette remarque aurait pu permettre de résoudre le 2) et 3) de l'exemple juste au dessus).
            \end{mdframed}

            \begin{mdframed}[linecolor=red]
                \textcolor{red}{\textbf{Théorème :}}

                Si $F_1$ et $F_2$ sont des sous-e.v. de E, alors $F_1 \cap F_2$
                est un sous-e.v. (attention, $F_1 \cup F_2$ ne l'est pas forcément).
            \end{mdframed}

            \begin{mdframed}[linecolor=green]
                \textcolor{green}{\textbf{Exemple :}}

                Soient $F_1 = \{(x, y) \in \R^2 \quad t.q. \; x = 0 \}$ et
                $F_2 = \{(x,y) \in \R^2 \quad t.q. \; y = 0 \}$. \\
                \includegraphics[scale=0.6]{figures/vecteurs.png}\\
                $F_1 \cap F_2$ est ici l'intersection des droites $x = 0$ et $y = 0$ qui est le singleton (ensemble de cardinal = 1)
                $(0;0)$. L'union ($\cup$) est par exemple la somme du vecteur $v (1, 0)$ et $w (0, 1)$.
                $v + w = (1, 1) \notin F_1$ ni $F_2$. Ce n'est donc pas un sous-e.v. de $\R$.
            \end{mdframed}

            \begin{mdframed}[linecolor=red]
                \textcolor{red}{\textbf{Théorème :}}

                Si $(\texttt{F}_i )_{i \in I}$ ($I$ = n'importe quel ensemble) est
                une famille quelconque de sous-e.v., $\bigcap_{i \in I} F_i$ est un sous-e.v.
            \end{mdframed}

            \bigskip

        \subsection{Applications linéaires}

            \begin{mdframed}[linecolor=blue]
                \textcolor{blue}{\textbf{Définition :}}

                Soient E et \texttt{F}, 2 e.v. sur le même corps $\mathbb K$.

                Une application $u : E \to \texttt{F}$ est \textbf{linéaire}
                si elle préserve les structures vectorielles, c'est a dire les propriétés suivantes :

                \[
                    \left \{
                        \begin{aligned}
                            u(x + y) &= u(x) + u(y) \; \forall x, y \in E\\
                            u(\lambda \cdot x) &= \lambda \cdot u(x) \; \forall \lambda \in \mathbb K, x \in E
                        \end{aligned}
                    \right.
                \]
            \end{mdframed}

            \begin{mdframed}[linecolor=green]
                \textcolor{green}{\textbf{Exemple :}}

                Les application suivantes sont elles linéaires?

                \begin{itemize}
                    \item $a : \R^2 \rightarrow \R^2 : (x, y) \mapsto (y, x)$
                    \item $b : \R^2 \rightarrow \R^2 : (x, y) \mapsto (\sin(x), y)$
                \end{itemize}

                1) Si $a$ est linéaire, elle doit respecter les 2 propriétés énoncées plus haut.
                Vérifions la première : $a(x + y) \stackrel{?}{=} a(x) + a(y)$.

                Considérons $(x, y)$ et $(v, w)$, 2 antécédents de $a$.
                Pour vérifier la première propriété, il faut que $a((x, y) + (v, w)) = a(x, y) + a(v, w)$.
                Développons des 2 côtés:

                (Gauche) $a((x, y) + (v, w)) = a(x + v, y + w)$

                Maintenant on applique $a$ qui va intervertir les coordonnées.

                $a(x + v, y + w) = (y + w, x + v)$

                (Droite) $a(x, y) + a(v, w) = (y, x) + (w, v) = (y + w, x + v)$.

                Le coté gauche de l'équation est égal au coté droit, donc la première propriété est démontrée.
                Maintenant il nous faut démontrer la seconde propriété :
                $a(\lambda \cdot x) \stackrel{?}{=} \lambda\cdot a(x)$

                On choisit un couple $(x, y)$, antécédent de $a$. Une fois encore,
                on vérifie en développant des 2 côtés et on regarde s'ils sont égaux.
                Donc si $a(\lambda \cdot (x, y))= \lambda \cdot a(x, y)$.

                (Gauche) : $a(\lambda \cdot (x, y)) = a(\lambda x, \lambda y) = (\lambda y, \lambda x)$

                (Droite): $\lambda \cdot a(x, y) = \lambda \cdot (y, x) = (\lambda y, \lambda x)$.

                Les côtés gauche et droit sont égaux, la seconde propriété est donc respectée.
                Comme les 2 propriétés ont été vérifiées et approuvées, $a$ est linéaire.

                2) (Même principe que plus haut, donc voir $a$ pour les détails)

                Si $b$ est linéaire elle doit respecter les 2 propriétés énoncées plus haut.
                Vérifions la première: $b(x + y) \stackrel{?}{=} b(x) + b(y)$ et
                donc $b((x, y) + (v, w)) = b(x, y) + b(v, w)$

                (Gauche) : $b((x, y) + (v, w)) = b(x + v, y + w) = (\sin(x + v), y + w)$

                (Droite) : $b(x, y) + b(v, w) = (\sin(x), y) + (\sin(v), w) = (\sin(x) + \sin(v), y + w)$

                Or, $\sin(x + v) \neq \sin(x) + \sin(v) \forall x, v \in \R$,
                donc l'égalité est fausse. Comme nous avons une contradiction
                il n'est pas utile de vérifier la seconde propriété. $b$ n'est
                pas linéaire. Il suffit de trouver un \textbf{contre-exemple}
                pour prouver qu'une hypothèse est fausse.
                Si l'on en trouve pas, il faut démontrer que les propriétés marchent dans notre cas.
                (Voir première séance d'exercice pour plus d'exercice sur les applications linéaires)
            \end{mdframed}

            \begin{mdframed}[linecolor=cyan]
                \textcolor{cyan}{\textbf{Remarque :}}

                Si $u : E \rightarrow F$ est une application linéaire, alors
                $u(0_E)=0_F$. Autrement dit, si le domaine de l'application ne
                contient pas l'élément neutre, alors elle est d'office pas linéaire
                (\textbf{Attention} cette affirmation n'est pas tout le temps vraie dans l'autre sens !).
            \end{mdframed}

            \bigskip

            \begin{mdframed}[linecolor=red]
                \textcolor{red}{\textbf{Théorème :}}

                Soit $A$, un ensemble quelconque, soit E un $\mathbb K$-e.v.\\
                On écrit $E^A = \{u : A \to E\}$
                l'ensemble des applications allant de $A$ dans $E$. $E^A$ a une structure vectorielle.
            \end{mdframed}

            \bigskip

            \begin{mdframed}[linecolor=blue]
                \textcolor{blue}{\textbf{Définition :}}

                Une application linéaire qui est bijective (injective et surjective) est appelée \textbf{isomorphisme}.

                Rappel pour $f : E \rightarrow F$ :

                \begin{itemize}
                    \item f est injective $\Leftrightarrow$ il n'existe pas 2 points qui ont la même image.
                          Autrement dit, $f(v) = f(w) \Rightarrow v = w$. \\
                          ex : $f : \R \rightarrow \R : x \mapsto x^2$
                          n'est pas injective sur $\R$ car pour $f(x) = 1$, on a
                          $x^2 = 1 \Leftrightarrow x \in \{-1, 1\}$,
                          mais elle est injective sur $\R^+$ ;
                    \item f est surjective $\Leftrightarrow$ tout les points de l'ensemble des images sont atteints.
                          Autrement dit, $\forall w \in F, \exists v \in E t.q. \; f(v) = w$. \\
                          ex : $f: \R \rightarrow \R : x \mapsto  x^2$
                          n'est pas surjective sur l'ensemble image $\R$ car
                          $\nexists x t.q. \; f^2 = -1$. Mais elle est surjective
                          sur l'ensemble des images $\R^+$ ;
                    \item f est bijective $\Leftrightarrow$ l'application est à la fois injective et surjective.
                          ex : $f : \mathbb \rightarrow \R : x \mapsto x^3$.
                \end{itemize}

                Avec un schéma:

                \includegraphics[scale=0.475]{figures/bijection.png}
            \end{mdframed}

            \begin{mdframed}[linecolor=green]
                \textcolor{green}{\textbf{Exemple :}}

                Comment prouver qu'une application est bijective?

                $u :\R^3 \to \R^3 : (x, y, z)\mapsto: (x+y-z, x-y, z)$

                Si $u$ est bijective, alors pour $(a, b, c)$ fixé, il existe un
                unique élément $(x, y, z) \in \R^3$ tel que $u(x, y, z) = (a, b, c)$.
                Nous avons donc un système d'équations !

                \[
                    \left \{
                        \begin{aligned}
                            x + y - z &= a \\
                            x - y &= b\\
                            x &= c
                        \end{aligned}
                    \right.
                \]
                \[
                    \left \{
                        \begin{aligned}
                             x &= c \\
                             y &= -b + c\\
                             z &= -a - b + 2c
                        \end{aligned}
                    \right.
                \]

                Il existe donc qu'une et une seule solution au système, donc l'application $u$ est bijective.
                Si elle est également linéaire (ce qu'elle est, la démonstration est laissée en exercice),
                alors $u$ est un isomorphisme.
            \end{mdframed}

            \begin{mdframed}[linecolor=red]
                \textcolor{red}{\textbf{Théorème :}}

                $\mathbb K^{\{0, 1\}} \sim \mathbb K^2 $ ($\sim$ veut dire \textit{est comparable à}),
                c'est-à-dire qu'il existe un \textbf{isomorphisme canonique} :
                $\phi : \mathbb K^{\{0, 1\}} \to \mathbb K^2$.

                Soit $A$ un ensemble fini : \#$A$ = $n$ \quad(\# = cardinal = nombre d'éléments).
                $\mathbb K^A \sim \mathbb K^n \longrightarrow$ ils sont canoniquement isomorphes.
            \end{mdframed}

            \begin{mdframed}[linecolor=red]
                \textcolor{cyan}{\textbf{Remarque :}}

                Y a t-il une relation entre sous-espace vectoriel et application linéaire?

                Soit $u : E_1 \to E_2$, une application linéaire.

                \begin{itemize}
                    \item Soit $F_1$, un sous-e.v de $E_1$, $u(F_1) = \{u(v) \quad t.q. \; v \in F_1 \} (\subset E_2)$
                          est un sous-e.v. de $E_2$ ;
                    \item soit $F_2$, un sous-e.v de $E_2$, $u^{-1}(F_2) = \{v \in E \quad t.q. \; u(v) \in F_2 \}$
                          est sous-e.v de E$_1$
                \end{itemize}
            \end{mdframed}

            \subsubsection{Noyau et Image}
                \begin{mdframed}[linecolor=blue]
                    \textcolor{blue}{\textbf{Définition :}}

                    Pour toute application linéaire $u : E_1 \rightarrow E_2$,
                    on définit $\Ker u$ et $\ImAppli u$, tels que \\
                    $\Ker u = u^{-1}(\{0_{E_2}\})$ est le \textit {noyau} de $u$ (sous-e.v. de E$_1$), \\
                    $\ImAppli u = u(E_1)$ est  l'\textit{image} de $u$ (sous-e.v. de E$_2$).

                    Autrement dit, $\Ker u = \{v \in E_1 t.q. u(v) = 0_{E_2}\}$, ce qui signifie concrètement :
                    \textit{l'ensemble des éléments qui sont envoyé sur l'élément neutre de l'ensemble d'arrivée}.\\
                    $\ImAppli u = \{v \in E_2, \exists w \in E_1 t.q. u(w) = v\}$.
                    C'est donc le sous-ensemble de $E_2$ contenant les images de tous
                    les éléments de $E_1$.
                \end{mdframed}

                \begin{mdframed}[linecolor=green]
                    \textcolor{green}{\textbf{Exemple :}} (Ex 2 fiche 2)

                    On fixe un vecteur $v_0 \in \R^3$. On définit l'application
                    $\Phi_1 : \R^3 \to \R^3 : v \mapsto v \times v_0$.
                    Déterminer le noyau $\Ker$ et l'image $\ImAppli$ de $\Phi _1$.

                    2 méthodes sont possibles :

                    \begin{enumerate}
                        \item Par calculs :

                        Soit $v_0 = (v_{0_x},v_{0_y},v_{0_z} )$ fixé.

                        $\Phi_1$ est le produit vectoriel de 2 vecteurs. Son noyau est
                        donc l'ensemble des vecteurs $v \in \R^3$ tels que $v \times v_0 = (0, 0, 0)$. \\
                        Or $v \times v_0  = (v_y v_{0_z} - v_{0_y} v_z, v_z v_{0_x} - v_x v_{0_z}, v_x v_{0_y} - v_y v_{0_x})$.

                        \[
                            v \times v_0  = (0, 0, 0) \\
                            \Leftrightarrow
                            \left \{
                                \begin{aligned}
                                    v_y v_{0_z} - v_z v_{0_y} &= 0 \\
                                    v_z v_{0_x} - v_x v_{0_z} &= 0 \\
                                    v_x v_{0_y} - v_y v_{0_x} &= 0 \\
                                \end{aligned}
                            \right .
                            \Leftrightarrow
                            \left \{
                                \begin{aligned}
                                    v_x &= v_z \frac{v_{0_x}}{v_{0_z} } \\
                                    v_y &= v_z \frac{v_{0_y}}{v_{0_z} } \\
                                    0 = 0 \\
                                \end{aligned}
                            \right .
                        \]

                        Nous n'avons donc pas de contrainte pour $v_z$, il est donc
                        toujours bon, peu importe sa valeur. (degré de liberté).

                        $S = \{z \frac{v_{0_x}}{v_{0_z}}, z \frac{v_{0_y}}{v_{0_z}}, z\} = z \{\frac{v_{0_x}}{v_{0_z}}, \frac{v_{0_y}}{v_{0_z}}, 1\}$.

                        $\Ker \Phi_1 = \R \{\frac{v_{0_x}}{v_{0_z}}, \frac{v_{0_y}}{v_{0_z}}, 1\} = \R \frac 1{v_{0_z}} \{v_{0_x}, v_{0_y}, v_{0_z}\}$
                        qui est la droite vectorielle de $\R$ engendrée par $v_0$.

                        \item En utilisant la définition du produit vectoriel (rappel : $v \times w = 0_{\R^3} \Leftrightarrow v = \lambda w$):

                        $\Ker \Phi _1 = \{v \in \R^3 | \Phi _1(v) = v \times v_0 = 0_{\R^3}\} = \{v \in \R^3 | v = \lambda v_0\} = \{\lambda v_0 \quad \forall \lambda \in \R\} = \R v_0$
                        $\R v_0$ est la droite vectorielle de $\R$ engendrée par  $v_0$.

                        $\ImAppli \Phi_1 = \{w \in \R^3 \text{pour lesquels} \, \exists v \in \R^3 | w = v \times v_0\} = \{ w \in \R^3 | (w \perp v) \land (w \perp v_0)\}$,
                        ce qui est le plan perpendiculaire à $v_0$.
                    \end{enumerate}
                \end{mdframed}

            \subsubsection{Combinaison linéaire}
                \begin{mdframed}[linecolor=blue]
                    \textcolor{blue}{\textbf{Définition :}}

                    Soient $v_1, \ldots ,v_n \in E$ (vecteurs) et $\lambda_1, \ldots ,\lambda_n \in \mathbb K$ (scalaires).
                    On dit que $\displaystyle \sum_{i=1}^n \lambda_i v_i = \lambda_1 v_1 + \ldots + \lambda_i v_i $
                    est une \textbf{combinaison linéaire} (c.l.) des vecteurs  $v_1, \ldots ,v_n$.
                \end{mdframed}

                \begin{mdframed}[linecolor=red]
                    \textcolor{red}{\textbf{Théorème :}}

                    $u: E_1 \to E_2$ est linéaire, ce qui signifie que pour toute
                    c.l. $\displaystyle \sum_{i=1}^n \lambda_i v_i$, on a
                    $u\left(\displaystyle \sum_{i=1}^n \lambda_i v_i \right) = \displaystyle \sum_{i=1}^n \lambda_i \ u(v_i)$.

                    $F \subset E$ (avec $E$ e.v. et $F$ sous-e.v. de E), signifie que toute
                    c.l. de $F$, $\displaystyle \sum_{i=1}^n \lambda_i v_i \in F$ pour
                    $\lambda_i, \ldots, \lambda_n \in \mathbb K$, et $ v_1, \ldots, v_n \in F$.
                \end{mdframed}

                \bigskip

                \begin{mdframed}[linecolor=blue]
                    \textcolor{blue}{\textbf{Définition :}}

                    Soit $A \subset E$, un espace quelconque compris dans $E$,
                    $\Vect A$ (aussi noté $\langle A \rangle$) est défini ainsi :

                    $\Vect A \equiv \displaystyle \bigcap_{\substack{F, sous-e.v. de E \\ t.q. A \subset F}} F \overset{lemme}{=} \{\displaystyle \sum_{i=1}^n \lambda_i v_i \quad \forall (\lambda_i, v_i) \in \mathbb K \times A\}$
                    est l'ensemble de toutes les c.l possibles sur base des vecteurs de A.

                    $v \in \Vect A \Leftrightarrow \exists (\lambda_1, \ldots, \lambda_n) \in \mathbb K^n | v = \displaystyle \sum_{i=1}^2 \lambda_i a_i$ avec $a \in A$.
                    $\Vect A$ est un sous e.v. de $E$ et on l'appelle l'\textbf{espace vectoriel engendré} par $A$.

                    Pour prouver ce lemme, il faut prouver que $\displaystyle \bigcap_{\substack{F, sous-e.v. de E \\ t.q. A \subset F}} F$ et
                    $\{\displaystyle \sum_{i=1}^n \lambda_i v_i \quad \forall (\lambda_i, v_i) \in \mathbb K \times A\}$ sont égaux.
                    Il est possible d'utiliser le théorème de double inclusion pour cela.
                    Rappel : soient $A$ et $B$, 2 sous-ensembles de E. $A = B \Leftrightarrow (A \subset B) \land (B \subset A)$.
                \end{mdframed}

        \subsection{Base et dimension}
            \subsubsection{Famille génératrice}
                \begin{mdframed}[linecolor=blue]
                    \textcolor{blue}{\textbf{Définition :}}

                    Soit $A \subset E$, on dit que $A$ est une \textbf{famille génératrice}
                    (ou partie génératrice) si $\Vect A = E$. Autrement dit,
                    $\forall v \in E, \exists \lambda_1, \ldots, \lambda_n \in \mathbb K, v_1, \ldots, v_n \in A \ | \  v = \displaystyle \sum_{i=1}^n \lambda_i v_i$
                \end{mdframed}

                \begin{mdframed}[linecolor=blue]
                    \textcolor{blue}{\textbf{Lemmes :}}

                    \begin{itemize}
                        \item Si $A$ est génératrice et $A \subset B$, alors $B$ est génératrice.
                        \item Soit $A$, une famille génératrice et $v \in A$,
                              alors $A \setminus \{v\}$ est génératrice si et seulement
                              si $v \in \Vect A \setminus \{v\}$
                    \end{itemize}
                \end{mdframed}

                \begin{mdframed}[linecolor=green]
                    \textcolor{green}{\textbf{Exemple :}}

                    Prenons $E = \R^3$. La famille $A = \{(0, 1, 0), (0, 0, 1), (1, 1, 1)\} \subset \R^3$ est-elle génératrice?

                    Soit un vecteur quelconque $(a, b, c)$, $\exists ? (x, y, z)$ tel que
                    $(a, b, c) \stackrel{?}{=} x(0, 1, 0) + y(0, 0, 1)+z(1, 1, 1)$ ?

                    Ceci est équivalent à trouver une solution au système suivant :

                    \[
                        \left \{
                            \begin{aligned}
                                z &= a \\
                                x+z &= b \\
                                y+z &= c \\
                            \end{aligned}
                        \right.
                        \Leftrightarrow
                        \left \{
                            \begin{aligned}
                            x &= b-a \\
                            y &= c-a \\
                            z &= a \\
                            \end{aligned}
                        \right.
                    \]

                    Étant donné que pour tout vecteur $(a, b, c)$, $\exists x, y, z \in \R \ | \  (a, b, c) = x(0, 1, 0) + y(0, 0, 1)+z(1, 1, 1)$,
                    on peut affirmer que $A$ est génératrice.
                \end{mdframed}

            \subsubsection{Famille libre/liée}
                \begin{mdframed}[linecolor=blue]
                    \textcolor{blue}{\textbf{Définition :}}

                    Soit $A \subset E$, un sous-ensemble de E qulconque, on dit que $A$
                    est \textbf{libre} si la seule c.l. de vecteurs $\in A$ égale à $0_E$ est la c.l. nulle triviale.
                    C'est à dire si $\displaystyle \sum_{i=1}^n \lambda_i v_i = 0_E$, avec $v_1, \ldots, v_n \in A \Rightarrow \lambda_1 = \lambda_2 = \ldots =  0$

                    Si une famille n'est pas libre, elle est \textbf{liée}.
                \end{mdframed}

                \begin{mdframed}[linecolor=green]
                    \textcolor{green}{\textbf{Exemple :}}

                    \begin{enumerate}
                        \item Prenons $E = \R^2, A = \{(-2, 3), (2, -3)\}$. A est liée car
                              $v_1 + v_2 = (-2, 3) + (2, -3) = (-2, 3) + (2, -3) = (0, 0) = 0_E$. \\
                              A est donc liée car on a trouvé $\lambda_1 = \lambda_2 = 1 \neq 0$
                        \item Prenons $E = \R^3, A' = \{(1, 1, 3),  (5, 2, -1), (4, 1, -4)\}$.
                              $A'$ est liée aussi car  $v_1 - v_2 + v_3 = 0_E$. \\
                              $A'$ est donc liée car on a trouvé $(\lambda_1, \lambda_2, \lambda_3) = (1, -1, 1) \neq (0, 0, 0)$
                        \item Prenons $E = \R^3, A'' = \{(1, 1, 3), (5, 2, -1)\}$.
                              À vu d'\oe{}il on ne sait pas dire si $A''$ est libre ou liée.
                              Donc on résout le système, il faut donc essayer de trouver une c.l.
                              des éléments de $A''$ égale à $0_E$.
                              C'est à dire : $\lambda_1(1, 1, 3) + \lambda_2(5, 2, -1) = 0_E = (0, 0, 0)$

                              Ce qui revient à résoudre le système:
                              \[
                                \left \{
                                    \begin{aligned}
                                        \lambda_1 + 5\lambda_2 &= 0 \\
                                        \lambda_1 + 2\lambda_2 &= 0 \\
                                        3\lambda_1 - \lambda_2 &= 0 \\
                                    \end{aligned}
                                \right .
                                \Leftrightarrow
                                \left \{
                                    \begin{aligned}
                                        \lambda_1 &= -5\lambda_2 \\
                                        \lambda_1 &= -2\lambda_2 \\
                                        \lambda_1 &= \frac{\lambda_2}{3} \\
                                    \end{aligned}
                                \right .
                                \Leftrightarrow
                                \lambda_1 = \lambda_2 = 0
                              \]
                              Donc $A''$ est libre!
                    \end{enumerate}

                    Si on avait essayé de résoudre le système du 2, on aurait obtenu le calcul suivant :

                    \[
                        \left \{
                            \begin{aligned}
                                \lambda_1 + 5\lambda_2 + 4\lambda_3 &= 0 \\
                                \lambda_1 + 2\lambda_2 + \lambda_3 &= 0 \\
                                3\lambda_1 - \lambda_2 - 4\lambda_3 &= 0 \\
                            \end{aligned}
                        \right .
                        \Leftrightarrow
                        \left \{
                            \begin{aligned}
                                \lambda_1 &= \lambda_3 \\
                                \lambda_2 &= -\lambda_3 \\
                            \end{aligned}
                        \right .
                        \Leftrightarrow
                        \lambda_1 = -\lambda_2 = \lambda_3
                    \]

                    Donc $\forall \lambda, \lambda v_1 - \lambda v_2 + \lambda v_3 = 0_E$.
                    La réponse (contrexemple) que nous avions trouvée :
                    $v_1 - v_2 + v_3 = 0_E$ correspondant au vecteur (1, -1, 1)
                    et correspond donc bien à une de nos réponses du système (avec $\lambda = 1$).
                \end{mdframed}

                \begin{mdframed}[linecolor=blue]
                    \textcolor{blue}{\textbf{Définition :}}

                    Si $A$ est libre, on dit que les vecteurs $\in A$ sont
                    \textbf{linéairement indépendant} (l.i), et si $A$ est liée,
                    alors on dit que ses vecteurs sont \textbf{linéairement dépendant} (l.d.).
                \end{mdframed}

                \begin{mdframed}[linecolor=cyan]
                    \textcolor{cyan}{\textbf{Remarque :}}

                    Si $0_E \in A$, $A$ ne peux pas être libre (donc elle est d'office liée) car
                    $\lambda 0_E = 0_E \qquad \forall \lambda$.
                \end{mdframed}

                \begin{mdframed}[linecolor=blue]
                \textcolor{blue}{\textbf{Lemmes :}}

                    Soit $A$, une famille de E.

                    \begin{itemize}
                        \item $A \subset B$ est liée $\Rightarrow B$ est liée ;
                        \item $A \supset B$ est libre $\Rightarrow B$ est libre ;
                        \item $A$ est libre et $v \in \Vect A \Rightarrow \exists! (\lambda_1, \ldots, \lambda_n) \in \mathbb K^n \ | \  v = \displaystyle \sum_{i=1}^n \lambda_i v_i$
                              avec $v_1, \ldots, v_n \in A$ ;
                        \item si $A$ est libre et $v \in E \setminus {A}$, alors $A \cup \{v\}$
                              est libre si et seulement si $v \notin \Vect A$.
                    \end{itemize}
                \end{mdframed}

                \begin{mdframed}[linecolor=red]
                    \textcolor{red}{\textbf{Théorème :}}

                    Si $A \subset E$ est libre, alors $v \in A \Rightarrow A \setminus \{v\}$ est libre et
                    $(v \notin A) \land (A \cup \{v\}$ est libre$) \Leftrightarrow v \notin \Vect A$.

                    Si $A \subset E$ est génératrice, alors $(v \in A) \land (A \setminus \{v\}$ est génératrice$) \Leftrightarrow v \in \Vect A \setminus \{v\}$ et
                    $v \notin A \Rightarrow A \cup \{v\}$ est génératrice.
                \end{mdframed}

            \subsubsection{Bases et dimensions}  % Bases & dimensions
                \begin{mdframed}[linecolor=blue]
                    \textcolor{blue}{\textbf{Définition :}}

                    Soit $B \subset E$, une partie de $E$. Si $B$ est libre et génératrice,
                    on dit que $B$ est une \textbf{base} de $e$.

                    Soit $E$, un $\mathbb K$-e.v. alors $E$ admet une base (voire même plusieurs).
                    Toutes les bases de E ont le même nombre d'éléments.
                    C'est ainsi que l'on définit la dimension d'un espace vectoriel : $\dim E = \# B$.
                \end{mdframed}

                \begin{mdframed}[linecolor=green]
                    \textcolor{green}{\textbf{Exemple :}}

                    \begin{itemize}
                        \item $\dim \R^n = n$ ;
                        \item $\dim \R^\mathbb N = \infty$ (car $\# \mathbb N = \infty$) ;
                        \item $v \neq 0 \in E \Rightarrow \dim v\mathbb K = \dim \Vect \{v\} = 1$ ;
                        \item $\dim \{0_E\} = 0$ (par convention).
                    \end{itemize}
                \end{mdframed}

                \begin{mdframed}[linecolor=red]
                    \textcolor{red}{\textbf{Théorème :}}

                    Si $A \subset E$ est libre, $\exists B \supset A$, une base de $E$.
                    Inversement, si $A \subset E$ est génératrice, $\exists B \subset A$, une base de $E$.
                    Autrement dit, une famille qui est libre/génératrice mais qui n'est pas génératrice/libre
                    peut devenir une base (donc libre et génératrice) à condition que l'on rajoute/enlève
                    des vecteurs correctement.
                \end{mdframed}

                \begin{mdframed}[linecolor=green]
                    \textcolor{green}{\textbf{Exemple :}}

                    $A = \{(1, 0, 0)\}$ est libre mais pas génératrice. Donc on va essayer
                    de rajouter des vecteurs dans $A$ pour la rendre génératrice, en restant libre.

                    $A' = \{(1, 0, 0), (1, 5, 3)\}$ est toujours libre et toujours pas génératrice.

                    $A''= \{(1, 0, 0), (1, 5, 3), (2, 5, 3)\}$ n'est plus libre
                    car $\exists$ une c.l nulle non triviale : $v_3 - v_1 - v_2 = 0$.
                    Il faut donc faire attention aux vecteurs que l'on ajoute,
                    car même si la famille était génératrice, elle ne peut pas être une base
                    car $A''$ n'est plus libre.

                    $A = \{(1, 0, 0), (0, 1, 0), (0, 0, 1), (1, 1, 1), (4, 3, 21)\}$ est génératrice
                    mais pas libre. On va donc essayer d'enlever des vecteurs de $A$
                    pour qu'elle devienne libre, tout en restant génératrice.

                    $A' = \{(1, 0, 0), (0, 1, 0), (0, 0, 1), (1, 1, 1)\}$ est toujours génératrice et toujours pas libre

                    $A'' = \{(1, 0, 0), (0, 1, 0), (0, 0, 1)\}$ est génératrice et libre, c'est donc une base.
                \end{mdframed}

                \begin{mdframed}[linecolor=red]
                    \textcolor{red}{\textbf{Théorème :}}\\
                    Soient $B$ et $B'$, deux bases de E, alors $\#B = \#B'$.
                \end{mdframed}

                \bigskip

                \begin{mdframed}[linecolor=blue]
                    \textcolor{blue}{\textbf{Rappel :}}

                    Soient $E_1, E_2$, deux $\mathbb K$-e.v.
                    $(E_2)^{E_1} = \{f : E_1 \rightarrow E_2 \}$ est un $\mathbb K$-e.v.
                \end{mdframed}

                \begin{mdframed}[linecolor=blue]
                    \textcolor{blue}{\textbf{Définition :}}

                    $L(E_1, E_2) \subset  (E_2)^{E_1}$, $L(E_1, E_2) = \{u : E_1 \rightarrow E_2 \ | \ u \ \text{linéaire}\}$.
                \end{mdframed}

                \begin{mdframed}[linecolor=red]
                    \textcolor{red}{\textbf{Théorèmes :}}

                    \begin{itemize}
                     \item $L(E_1, E_2)$ est un sous e.v. de $(E_2)^{E_1}$ ;
                     \item Si $\dim(E_1) < +\infty$ et $\dim(E_2) < +\infty$,
                           $\dim(L(E_1, E_2)) = \dim(E_1) \times dim(E_2)$ ;
                     \item Si $\#A < +\infty$ et $E$ est un e.v. de dimension finie,
                           $\dim(E^A) = (\dim(E))^{\#A}$.
                    \end{itemize}
                \end{mdframed}

                \begin{mdframed}[linecolor=red]
                    \textcolor{red}{\textbf{Théorème :}}

                    Soit E un $\mathbb K$-e.v. ed dimension $n$, $B = \{e_1, \ldots, e_n\}$, une base de E. \\
                    $\phi: \mathbb K^n \rightarrow E : (\lambda_1, \ldots, \lambda_n) \mapsto \displaystyle \sum_{i=1}^n \lambda_i e_i$
                    est un isomorphisme.
                \end{mdframed}

    \section{Relations d'équivalence}
        \begin{mdframed}[linecolor=blue]
            \textcolor{blue}{\textbf{Définition :}}

            Une \textbf{relation d'équivalence} dans un ensemble $A$ (quelconque)
            est une \textbf{relation binaire} (relation entre paires d'éléments) de $A$, notée $\sim$.

            Une relation d'équivalence doit être :

            \begin{enumerate}
                \item \textbf{Réflexive} : $x \sim x \quad \forall x \in A$
                      \textit{(tout le monde est en relation avec soi-même)} ;
                \item \textbf{Symétrique} : $x \sim y \Leftrightarrow  y \sim x \quad \forall x, y \in A$
                      \textit{(si $x$ est en relation avec $y$, alors $y$ est en relation avec $x$)}
                \item \textbf{Transitive} : $(x \sim y) \land (y \sim z) \Rightarrow x \sim z \quad \forall x, y, z \in A$
                      \textit{(les amis de mes amis, sont mes amis)}
            \end{enumerate}
            \begin{center}
                \includegraphics[scale = 0.66]{figures/equiv.png}
            \end{center}
        \end{mdframed}

        \begin{mdframed}[linecolor=magenta]
            \textcolor{magenta}{\textbf{Notation :}}

            $x \sim_p y$ s'écrit aussi communément $x \equiv y[p]$
        \end{mdframed}

        \begin{mdframed}[linecolor=green]
            \textcolor{green}{\textbf{Exemple :}}

            La relation \textit{être parallèle} $\parallel$ est une relation d'équivalence pour
            l'ensemble des droites du plans.

            Preuve :

            \begin{enumerate}
                \item Réflexivité : Toute droite est parallèle à elle-même ;
                \item Symétrie : $d \parallel d' \Rightarrow d' \parallel d$ ;
                \item Transitivité : $(d \parallel d') \land (d' \parallel d'') \Rightarrow d \parallel d''$.
            \end{enumerate}

            Soit $p \in \mathbb N^*$. On dit que deux nombres entiers relatifs
            $x$ et $y$ sont \textit{congrus modulo $p$} et on note: $x \equiv y[p] \Leftrightarrow x-y$
            est un multiple de $p$.
            Prouver qu'il s'agit d'une relation d'équivalence.

            $x \equiv y[p]$ si $x-y$ est un multiple de $p$. Revient au même que : si $p$ divise $x-y$.
            Si c'est une relation d'équivalences, alors elle respecte les 3 propriétés :
            \begin{enumerate}
                \item Réflexivité : $x \sim x$\\
                      $x \stackrel{?}{\equiv} x[p] \rightarrow x - x = 0$ (qui est bien un multiple de $p$) ;
                \item Symétrie : $x \sim y \Rightarrow y \sim x $\\
                      $x \equiv y[p] \stackrel{?}{\Rightarrow} y \equiv x[p]$\\
                      $x \equiv y[p] \rightarrow x-y$ est multiple de p.\\
                      $y \equiv x[p] \rightarrow y-x$ est multiple de p ?
                      $(x-y) \div p = n \in \mathbb Z \Rightarrow (y-x) \div p = -n \in \mathbb Z$ ;
                \item Transitivité : $(x \sim y) \land (y \sim z) \Rightarrow x \sim z$\\
                      $x \sim y \rightarrow p$ divise $x-y \rightarrow x-y = q_1 \times p \quad q_1 \in \mathbb Z$\\
                      $y \sim z \rightarrow p$ divise $y-z \rightarrow y-z = q_2 \times p \quad q_2 \in \mathbb Z$.

                      En sommant les 2 équations :
                      $(x-y)+(y-z) = (q_1 \times p) + (q_2 \times p) \Leftrightarrow x-z = p \times (q_1 + q_2)$

                      Or $x \sim z \rightarrow x-z = p \times q_3 \rightarrow q_3 = (q_1+q_2) \in \mathbb Z$
            \end{enumerate}
            Il s'agit donc bien d'une relation d'équivalence.
        \end{mdframed}

        \begin{mdframed}[linecolor=blue]
            \textcolor{blue}{\textbf{Définition :}}

            Soit $A$ muni d'une \textbf{relation d'équivalence} $\sim$.
            La classe d'un élément $x \in A$ est le sous-ensemble de $A$ défini par
            $[x]_\sim = \{y \in A \ | \ x \sim y\}$.

            \begin{center}
                \includegraphics[scale = 0.66]{figures/equiv2.png}
            \end{center}
        \end{mdframed}

        \begin{mdframed}[linecolor=green]
            \textcolor{green}{\textbf{Exemple :}}


            Le parallélisme, sur l'ensemble des droites du plan à comme classe les pentes (toutes les droites de même pente sont parallèles).

            Prouvons que la relation de \textit{congruence modulo $p$} ci-dessus contient exactement $p$ classes d'équivalences.

            Sur l'ensemble $\mathbb Z$ des entiers relatifs, la congruence modulo $p$
            (pour un entier $p$ fixé) est une relation d'équivalence dont les classes forment
            le groupe cyclique $\mathbb Z/p \mathbb Z$. Qui correspond à : \\
            $\{x + p, x + 2p, \ldots, x - p, x - 2p, \ldots\} = \{x + kp \ |\  k \in \mathbb Z \} = x + p\mathbb Z $

            Il y a donc $p$ classes d'équivalence différentes :
            $[0] = [p] = [2p] = \ldots$, $[1] = [1+p] = [1+2p] = \ldots$, jusque $[p-1] = [2p-1] = [3p-1] = \ldots$
        \end{mdframed}

        \begin{mdframed}[linecolor=red]
            \textcolor{red}{\textbf{Théorème :}}

            L'ensemble des classes d'équivalences de $\sim$ sur $E$ forme une partition de $E$.
            $(A_i)_{i \in I}$ est une famille de parties de $E$ : $\forall i \in I, A_i \subset E$.
            On dit que $A_i$ est une partition de $E$ si :

            \begin{itemize}
              \item $A_i \cap A_j = \emptyset \Leftrightarrow i \neq j$ (pas d'intersection entre les familles) ;
              \item $\displaystyle \bigcup_{i \in I} A_i = E$ (l'union de toute les familles forme l'ensemble)
            \end{itemize}

            \begin{center}
                \includegraphics[scale = 0.7]{figures/partition.png}
            \end{center}
        \end{mdframed}

        \begin{mdframed}[linecolor=magenta]
            \textcolor{blue}{\textbf{Définition :}}

            $\{[x] \ | \ x \in A\} = A/\sim$ et est appelé \textit{quotient de $A$ par $\sim$} ou \textit{$A$ quotienté $\sim$}.
        \end{mdframed}

        \begin{mdframed}[linecolor=green]
            \textcolor{green}{\textbf{Exemple :}}


            $\mathbb Z/\sim_p = \{[0], [1], \ldots, [p-1]\} = \mathbb Z / p\mathbb Z$

            Pour $p = 2$, on a $\mathbb Z / 2\mathbb Z \approx \{[0], [1]\} = \{\{ \text{"pairs" (multiples de 2)}\}, \{\text{"impairs"}\}\}$

            Pour $p = 3$, on a donc $\{\{n\ | \ n \mod 3 = 0\}, \{n \ | \ n \mod 3 = 1\}, \{n \ | \ n \mod 3 = 2\}\}$.
        \end{mdframed}

        \begin{mdframed}[linecolor=red]
            \textcolor{red}{\textbf{Théorème :}}

            $E/F = E/\sim_F$, l'ensemble des classes d'équivalences, a une structure
            vectorielle canonique $\mathbb K$.
        \end{mdframed}

        \begin{mdframed}[linecolor=cyan]
            \textcolor{cyan}{\textbf{Remarque :}}

            Une autre relation d'équivalence:\\
            Soient $A$ et $B$, 2 ensembles. On dit que $A$ et $B$ ont "le même cardinal"
            s'il existe une bijection \\
            $f : a \longrightarrow B$. Cette relation est notée $\sim_\#$.
         \end{mdframed}

        \begin{mdframed}[linecolor=magenta]
            \textcolor{magenta}{\textbf{Notation :}}

            Si $\#A = \#\{1, \ldots, n\}$, on dit que $A$ est un ensemble fini et on note $\#A = n$
        \end{mdframed}

        \begin{mdframed}[linecolor=red]
            \textcolor{red}{\textbf{Théorème :}}

            On dit que $\#A \leq \#B$ si $\exists f : a \to B$, une injection.

            Théorème de Contor-Bernstein : $(\#A \leq \#B) \land (\#B \leq \#A) \Rightarrow \#A = \#B$.
        \end{mdframed}

        \begin{mdframed}[linecolor=blue]
            \textcolor{blue}{\textbf{Définition :}}

            Si $\#A = \#\mathbb N$, on dit que $A$ est \textbf{dénombrable}.\\
            Si $\#A = \#\R$, on dit que $A$ a la puissance du continu.
        \end{mdframed}

        \begin{mdframed}[linecolor=red]
            \textcolor{red}{\textbf{Propriétés :}}

            \begin{itemize}
                \item Si $\#A \leq \#\mathbb N$, soit $A$ est fini, soit $A$ est dénombrable ;
                \item Si $\#A = n$ et $\#B = m$, $\#(B^A) = m^n = \#B^{\#A} \quad (B^A = \{f : A \to B \})$ ;
                \item $\# \mathbb Z = \# \mathbb N$ (idée de preuve : associer
                      les éléments pairs des éléments de $\mathbb N$ aux éléments positifs de
                      $\mathbb Z$ et les impaires aux négatifs. Du coup $\mathbb Z$ est dénombrable,
                      $\mathbb N^2$ aussi et également $\mathbb Q$ ;
                \item $\R$ n'est pas dénombrable. Autrement dit, il n'existe pas de bijection \\
                      $f : \mathbb N \to \{0\ ;\ ...\ ;\ 9\}^\mathbb N \Rightarrow \#\{0\ ;\ ...\ ;\ 9\}^\mathbb N > \#\mathbb N$
             \end{itemize}
        \end{mdframed}

    \section{Applications et Matrices}
    	\subsection{Généralité sur les matrices}

            \begin{mdframed}[linecolor=blue]
                \textcolor{blue}{\textbf{Définition :}}

                Soient $u: E \rightarrow E' \quad$ avec E et E', 2 $\mathbb K$-e.v.,
                $B = \{e_1, \ldots, e_n\}$, une base de E,
                $B' = \{e'_1, \ldots, e'_m\}$, une base de E'.

                $u(\phi(\lambda_1, \ldots, \lambda_n)) = \displaystyle \sum_{i, j=1}^{n,m} \lambda_i\ a_{j, i}\ e'_j$ (voir théorème précédent)

                On définit $M = [a_{j,i}]_{\substack{1 \leq i \leq m \\ 1 \leq j \leq n}} =
                \begin{pmatrix}
                  a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
                  a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
                  \vdots  & \vdots  & \ddots & \vdots  \\
                  a_{m,1} & a_{m,2} & \cdots & a_{m,n}
                 \end{pmatrix}$

                 $M$ est la matrice associée à $u$ dans les bases $B$ et $B'$.
            \end{mdframed}

            \begin{mdframed}[linecolor=green]
                \textcolor{green}{\textbf{Exemple :}}

                Soit $u: \R^3 \to \R^2 : u(x,y,z) = (x+3y-z,y+2z)$

                Trouver la matrice associée à $u$ dans les bases canoniques.

                La base canonique de $\R^3$ est \{$e_1, e_2, e_3$\} = \{(1, 0, 0), (0, 1, 0), (0, 0, 1)\}

                La base canonique de $\R^2$ est \{$e'_1, e'_2$\} = \{(1, 0), (0, 1)\}

                Trouvons les images associées aux vecteurs de la base canonique de départ.
                \[
                    \begin{aligned}
                        u(e_1) = u(1, 0, 0) &= (1 + 3 \times 0, 0 + 2 \times 0) = (1, 0) = e'_1 \\
                        u(e_2) = u(0, 1, 0) &= (3, 1) = 3 \times e'_1 + e'_2 \\
                        u(e_3) = u(0, 0, 1) &= (-1, 2) = -e'_1 + 2 \times e'_2
                    \end{aligned}
                \]

                La matrice associée à $u$ est l'ensemble des résultats précédents mis en colonnes :

                $M_u =
                \bordermatrix{& u(e_1) & u(e_2) & u(e_3) \cr
                              & 1      & 3      & -1 \cr
                              & 0      & 1      & 2 \cr} =
                \begin{pmatrix}
                    1 & 3 & -1 \\
                    0 & 1 & 2
                \end{pmatrix}$

                Si on vérifie dans l'autre sens en utilisant la définition d'au-dessus,
                $u(\lambda_1, \lambda_2, \lambda_3) = \displaystyle \sum_{i=1}^3 \displaystyle \sum_{j=1}^2 \lambda_i \ a_{j,i}\ e'_j $.

                Si on décompose:
                \renewcommand{\labelitemi}{$\bullet$}

                \begin{itemize}
                    \item $i =1$
                          \begin{itemize}
                              \item $j = 1: \lambda_1 a_{1,1} e'_1 = \lambda_1 1 (1, 0) = (\lambda_1, 0)$
                              \item $j = 2: \lambda_1 a_{2,1} e'_2 = \lambda_1 0 (0, 1) = 0$
                          \end{itemize}
                    \item $i =2$
                          \begin{itemize}
                              \item $j = 1: \lambda_2 a_{1,2} e'_1 = \lambda_2 3 (1, 0) = (3\lambda_2, 0)$
                              \item $j = 2: \lambda_2 a_{2,2} e'_2 = \lambda_2 1 (0, 1) = (0, \lambda_2)$
                          \end{itemize}
                    \item $i=3$
                          \begin{itemize}
                              \item $j = 1: \lambda_3 a_{1,3} e'_1 = \lambda_2 (-1) (1, 0) = (-\lambda_3, 0)$
                              \item $j = 2: \lambda_3 a_{2,3} e'_2 = \lambda_2 2 (0, 1) = (0, 2\lambda_3)$
                          \end{itemize}
                \end{itemize}

                La somme des 6 lignes donne: $(\lambda_1 + 3\lambda_2 - \lambda_3, \lambda_2 + 2\lambda_3)$. \\
                $u(\lambda_1, \lambda_2, \lambda_3) = (\lambda_1 + 3 \lambda_2 - \lambda_3, \lambda_2 + 2\lambda_3)$
                Ce qui est bien se qu'on avait au départ (en remplaçant les $\lambda$ par $x$, $y$ et $z$)
            \end{mdframed}

            \begin{mdframed}[linecolor=magenta]
                \textcolor{magenta}{\textbf{Notation :}}

                Les matrices sont aussi notées $\MatK(m, n)$ avec : $m$,
                le nombre de colonnes, $n$, le nombre de lignes.
            \end{mdframed}

            \begin{mdframed}[linecolor=red]
                \textcolor{red}{\textbf{Théorème :}}

                Les matrices $\MatK(m, n)$ sont des $\mathbb K$-e.v.
                Chaque matrice à sa \textit{propre} application linéaire qui lui est associée.
            \end{mdframed}

        \subsection{Produit matriciel}

            \begin{mdframed}[linecolor=red]
                \textcolor{red}{\textbf{Théorème :}}

                Soient $u: E \rightarrow E'$ et $u': E' \rightarrow E''$ avec E, E' et E'' des $\mathbb K$-e.v.

                $(u \in L(E, E')) \land (u' \in L(E', E'')) \Rightarrow u' \circ u \in L(E,E'')$

                Soient $B = \{e_1, \ldots, e_n\}$, une base de E, $B' = \{e'_1, \ldots, e'_m\}$, une base de E',
                $B'' = \{e''_1, \ldots, e''_p\}$, une base de E''.

                $[a_{i, j}]$ est la matrice de $u$, $[b_{j, k}]$ est la matrice de $u'$, et
                $[c_{i, k}]$ est la matrice de $u' \circ u$.

                $(u' \circ u)(e_i) = ... = \displaystyle \sum_{k=1}^p \left(\underbrace{\displaystyle \sum_{j=1}^m a_{i,j}\cdot b_{j,k}}_{c_{i,k}}\right)e''_k$
            \end{mdframed}

            \begin{mdframed}[linecolor=blue]
                \textcolor{blue}{\textbf{Définition :}}

                On définit donc le \textbf{produit matriciel} comme l'opération suivante:
                $\MatK(m, n) \times \MatK(p, m) \to \MatK(p, n)$ (donc $p$ colonnes et $n$ lignes)

                Soient $[a_{j, i}]_{\substack{1 \leq i \leq n \\ 1 \leq j \leq m}}$ et $ [b_{i, k}]_{\substack{1 \leq k \leq p \\ 1 \leq l \leq n}}$,
                $[a_{j, i}] [b_{i,k}] = \left[\displaystyle \sum_{i=1}^n a_{j, i} b_{i, k} \right]_{\substack{1 \leq k \leq p \\ 1 \leq j \leq m}}$.
            \end{mdframed}

            \begin{mdframed}[linecolor=green]
                \textcolor{green}{\textbf{Exemples:}}

                \[
                    \begin{aligned}
                        \begin{pmatrix}
                            1 & 3 & -1 \\
                            0 & 1 & 2
                        \end{pmatrix}
                        \begin{pmatrix}
                            0 & 2 \\
                            2 & -1 \\
                            1 & 4
                        \end{pmatrix} &=
                        \begin{pmatrix}
                            0 \cdot 1 + 2 \cdot 3 + 1 \cdot (-1) & 2 \cdot 1 + (-1) \cdot 3 + 4 \cdot (-1) \\
                            0 \cdot 0 + 2 \cdot 1 + 1 \cdot 2 & 2 \cdot 0 + (-1) \cdot 1 + 4 \cdot 2 \\
                        \end{pmatrix} =
                        \begin{pmatrix}
                            5 & -5 \\
                            4 & 3 \\
                        \end{pmatrix} \\
                        \begin{pmatrix}
                            a & b \\
                            c & d
                        \end{pmatrix}
                        \begin{pmatrix}
                            e & f \\
                            g & h
                        \end{pmatrix} &=
                        \begin{pmatrix}
                            a \cdot e + b \cdot g & a \cdot f + b \cdot h \\
                            c \cdot e + d \cdot g & c \cdot f + d \cdot h
                        \end{pmatrix}
                    \end{aligned}
                \]
            \end{mdframed}

            \subsubsection{Propriétés du produit matriciel}
                Soient $A \in \Mat(m, n)\quad B \in \Mat(p, m)\quad C \in \Mat(r, p)$

                Le produit matriciel est :
                \begin{itemize}
                    \item PAS commutatif : $A \times B \neq B \times A$ (mais parfois c'est le cas) ;
                    \item associatif : $(A B) C = A (B C)$ ;
                    \item distributif à droite : $(A_1 + A_2) B = A_1 B + A_2 B$ ;
                    \item distributif à gauche : $B (A_1 + A_2) = B A_1 + B A_2$.
                \end{itemize}

            \subsubsection{Produit matriciel et composition}

                \begin{mdframed}
                    \textcolor{blue}{Définition :}

                    Soient $E_1$ et sa base $B_1$ et $E_2$ et sa base $B_2$.
                    On définit $\Phi_{B_1B_2} : \Mat(\dim E_1, \dim E_2) \to L(E_1, E_2)$,
                    une application qui envoie une matrice sur l'application linéaire qui lui est associée.
                \end{mdframed}

                \begin{mdframed}[linecolor=red]
                    \textcolor{red}{\textbf{Théorème :}}

                    $\Phi_{BB'} ([a_{j, i}] [b_{i, k}]) = \Phi_{B'B''}([b_{i, k}]) \circ \Phi_{BB'}([a_{j, i}])$

                    Faire la matrice associée à la composée de deux applications linéaires
                    revient à faire le produit des matrices des deux applications.
                \end{mdframed}

                \begin{mdframed}[linecolor=green]
                    \textcolor{green}{\textbf{Exemple :}}

                    Soit $u: \R^3 \to \R^3 : (x,y,z) \mapsto (y, z, 0)$.
                    Calculez $M_{(u\ \circ\ u)}$ et $(M_u M_u)$

                    $M_{(u\ \circ\ u)} :\\
                    (u \circ u)(x, y, z) = u(u(x, y, z)) = u(y, z, 0) = (z, 0, 0)\\
                    (u \circ u)(e1) = (u \circ u)(1, 0, 0) = (0, 0, 0) \text{(1ere colonne de} M_{(u\ \circ\ u)} \text{)} \\
                    (u \circ u)(e2) = (u \circ u)(0, 1, 0) = (0, 0, 0) \text{(2eme colonne de} M_{(u\ \circ\ u)} \text{)} \\
                    (u \circ u)(e3) = (u \circ u)(0, 0, 1) = (1, 0, 0) \text{(3eme colonne de} M_{(u\ \circ\ u)} \text{)} \\
                    M_{(u\ \circ\ u)} =
                    \begin{pmatrix}
                        0 & 0 & 1 \\
                        0 & 0 & 0 \\
                        0 & 0 & 0
                    \end{pmatrix}$

                    \bigskip
                    $M_u M_u :\\
                    (x, y, z) = x \cdot e_1 + y \cdot e_2 + z \cdot e_3 \\
                    (y,z,0) = y \cdot e_1 + z \cdot e_2\\
                    u(e_1) = u(1, 0, 0) = 0 \cdot e_1 + 0 \cdot e_2 =  0  = (0, 0, 0) \text{(1ere colonne de} M_u \text{)} \\
                    u(e_2) = u(0, 1, 0) = 1 \cdot e_1 + 0 \cdot e_2 = e_1 = (1, 0, 0) \text{(2eme colonne de} M_u \text{)} \\
                    u(e_3) = u(0, 0, 1) = 0 \cdot e_1 + 1 \cdot e_2 = e_2 = (0, 1, 0) \text{(3eme colonne de} M_u \text{)} \\
                    M_u =
                    \begin{pmatrix}
                        0 & 1 & 0 \\
                        0 & 0 & 1 \\
                        0 & 0 & 0
                    \end{pmatrix}\\
                    M_u \times M_u =
                    \begin{pmatrix}
                        0 & 1 & 0 \\
                        0 & 0 & 1 \\
                        0 & 0 & 0
                    \end{pmatrix}
                    \begin{pmatrix}
                        0 & 1 & 0 \\
                        0 & 0 & 1 \\
                        0 & 0 & 0
                    \end{pmatrix} =
                    \begin{pmatrix}
                        0 & 0 & 1 \\
                        0 & 0 & 0 \\
                        0 & 0 & 0
                    \end{pmatrix}$
                \end{mdframed}

                \begin{mdframed}[linecolor=cyan]
                    \textcolor{cyan}{\textbf{Remarque :}}

                    $M_{\underbrace{u \circ u \circ ... \circ u}_{n\ \textrm{fois}}} = (M_u)^n $

                    Calculer ma matrice définie par $n$ composées d'une application revient à calculer la matrice de l'application à la puissance $n$.
                \end{mdframed}

        \subsection{Matrice identité}
            \begin{mdframed}[linecolor=blue]
                \textcolor{blue}{\textbf{Définition :}}

                On définit la \textbf{matrice identité} de $\MatK (m,n)$, notée $I$ ou $I_n$ (avec $n$ la dimension) ou$Id$, comme:

                \begin{center} $\begin{pmatrix}
                    1 & 0 & \hdots & 0 \\
                    0 & 1 & \vdots & \vdots \\
                    \vdots & \hdots &\ddots & 0  \\
                    0 & \hdots & 0 &1
                \end{pmatrix}$ \end{center}

                La matrice identité est carrée (même nombre de lignes et de colonnes)
                et a une diagonale de 1, avec tout le reste des 0.

                Elle a comme particularité que toute matrice $M$ de même dimension
                multiplié par l'identité, redonne la matrice $M$ : $M I = I M = M$
            \end{mdframed}

            \begin{mdframed}[linecolor=green]
                \textcolor{green}{\textbf{Exemple :}}

                $I_{3 \times 3} =
                \begin{pmatrix}
                    1 & 0 & 0 \\
                    0 & 1 & 0 \\
                    0 & 0 & 1
                \end{pmatrix} \quad I_{4 \times 4} =
                \begin{pmatrix}
                    1 & 0 & 0 & 0 \\
                    0 & 1 & 0 & 0 \\
                    0 & 0 & 1 & 0 \\
                    0 & 0 & 0 & 1
                \end{pmatrix} \quad  I_{5 \times 5} =
                \begin{pmatrix}
                    1 & 0 & 0 & 0 & 0 \\
                    0 & 1 & 0 & 0 & 0 \\
                    0 & 0 & 1 & 0 & 0 \\
                    0 & 0 & 0 & 1 & 0 \\
                    0 & 0 & 0 & 0 & 1
                \end{pmatrix}$
            \end{mdframed}

            \begin{mdframed}[linecolor=cyan]
                \textcolor{cyan}{\textbf{Remarque :}}

                $I^n = I \quad \forall n \in \mathbb N^+$
            \end{mdframed}

        \subsection{Matrice inversible}
            \begin{mdframed}[linecolor=blue]
                \textcolor{blue}{\textbf{Définition :}}

                $M$ est inversible $\Leftrightarrow \exists M^{-1} \ t.q.\ M M^{-1} = M^{-1} = I$
            \end{mdframed}

            \begin{mdframed}[linecolor=red]
                \textcolor{red}{\textbf{Théorème :}}

                Si une matrice $M$ est \textbf{nilpotente} ($\exists n \ t.q.\ M^n = 0$),
                $M$ n'est pas inversible.
            \end{mdframed}

            \subsubsection{Calcul de puissance de matrice}
                En utilisant les propriétés des l'inversibilité des matrices,
                celles des matrices nilpotentes et le binôme de Newton, on
                peut calculer plus facilement des grandes puissance de matrices.

                \begin{mdframed}[linecolor=green]
                    \textcolor{green}{\textbf{Exemple :}}

                    Soit $M = \begin{pmatrix}
                        1 & 1 & 0 \\
                        0 & 1 & 1 \\
                        1 & 0 & 1
                    \end{pmatrix}$.

                    On définit $N$ tel que $M + I = N$. Donc $N = \begin{pmatrix}
                        0 & 1 & 0 \\
                        0 & 0 & 1 \\
                        1 & 0 & 0
                    \end{pmatrix}$

                    $N$ est nilpotent tel que $N^3 = I$. Calculer $M^{2015}$.

                    Si $N$ est nilpotent, alors :

                    \begin{itemize}
                        \item $N^{3k} (N^3)^k = I^k = I$ ;
                        \item $N^{3k+1} = (N^3)^k N = I N = N$ ;
                        \item $N^{3k+2} = (N^3)^k N = I N^2 = N^2$.
                    \end{itemize}

                    $M^n = (I + N)^n = \displaystyle \sum^n_{k=0} C_n^k \cdot N^k \cdot Id^{(n-k)} = \displaystyle \sum^n_{k=0} C_n^k \cdot N^k$

                    $M^{2015} = \displaystyle \sum^{2015 \div 3}_{p=0}\left(C_{2015}^{3p} \cdot Id + C_{2015}^{3p+1} \cdot N + C_{2015}^{3p+2} \cdot N^2 \right) =
                    \begin{pmatrix}
                        a & c & b \\
                        b & a & c \\
                        c & b & a
                    \end{pmatrix}$ avec $a = \displaystyle \sum^{671}_{p=0} C_{2015}^{3p}$,
                    $b = \displaystyle \sum^{671}_{p=0} C_{2015}^{3p+1}$, $c = \displaystyle \sum^{671}_{p=0} C_{2015}^{3p+2}$.
                \end{mdframed}

        \subsection{Retour sur les e.v.}

            \begin{mdframed}[linecolor=blue]
                \textcolor{blue}{\textbf{Définition :}}

                Soit $E$ un e.v. de dimension finie avec $B = \{e_1, \ldots, e_n\}$, famille de $E$.
                On définit $\Phi_B : \mathbb K^n \to E : (\lambda_1, \ldots ,\lambda_n) \mapsto \displaystyle \sum^n_{i=1} \lambda_i e_i$.

                On définit également $\Psi_B = \Phi_B^{-1}$, l'inverse de $\Phi_B$ tel que
                $\Psi_B : E \to \mathbb K^n$, or
                $\mathbb K^n \approx \MatK(n, 1) \approx \MatK(1, n)$.
                Effectivement, $\mathbb K^n$ est un vecteur de $n$ éléments de $\mathbb K$.
                Un vecteur n'est autre qu'un vecteur ligne ou un vecteur colonne.

                $v \mapsto \begin{pmatrix}\lambda_1 \\ \vdots \\ \lambda_n\end{pmatrix}$
                où $(\lambda_1, ... , \lambda_n)$ sont les coefficients de $v$ dans la base $B$

                Soit $E'$ un autre e.v. sur $\mathbb K$ avec $B' = \{e'_1, \ldots, e'_n\}$,
                une base de $E'$. $\Phi_{B'} : \mathbb K^m \to E'$ et $\Psi_{B'} : E' \to \mathbb K^m $

                Soit $u \in L(E, E')$. $\forall i, 1 \leq i \leq n, u(e_i) = \displaystyle \sum^m_{j=1} a_{j,i} e'_j =
                \begin{pmatrix} a_{1,i} \\ \vdots \\ a_{m,i} \end{pmatrix} = \Psi_{B'}(u(e_i)).$ \\
                $\forall v \in E = \displaystyle \sum^n_{i=1} \lambda_i e_i, \Phi_B(v) = \begin{pmatrix} \lambda_1 \\ \vdots \\ \lambda_n \end{pmatrix}$. \\
                $v' \in E' = u(v) = u \left(\displaystyle \sum^n_{i=1} \lambda_i e_i \right) =
                \displaystyle \sum^n_{i=1} \lambda_i u(e_i) =
                \displaystyle \sum^n_{i=1} \lambda_i \displaystyle \sum^n_{j=1} a_{j,i} e_j =
                \displaystyle \sum^n_{j=1} \left( \displaystyle \sum^n_{i=1} a_{j,i} \lambda_i \right)e'_j$

                $\Rightarrow \lambda'_j = \displaystyle \sum^n_{i=1} a_{j,i} \lambda_i,
                \Phi_{B'}(v')=
                \begin{pmatrix}
                    \displaystyle \sum^n_{i=1} a_{1,i} \lambda_i \\
                    \vdots \\
                    \displaystyle \sum^n_{i=1} a_{m,i} \lambda_i
                \end{pmatrix} = [a_{j,i}] [\lambda_i] =
                \begin{pmatrix}
                    \lambda'_1 \\
                    \vdots \\
                    \lambda'_m
                \end{pmatrix} =
                \begin{pmatrix}
                    a_{1,1} & \ldots & a_{1,n}  \\
                    \vdots & \ddots & \vdots\\
                    a_{m,1} & \ldots & a_{m,n}
                \end{pmatrix}
                \begin{pmatrix}
                    \lambda_1 \\
                    \vdots \\
                    \lambda_n
                \end{pmatrix}$

                Cette démonstration permet de relier les notions vues précédemment :
                nous pouvons donc appliquer une application linéaire quelconque à
                un vecteur uniquement à l'aide du produit vectoriel.

                Lorsque nous avons deux $\mathbb K$-e.v. $E$ et $E'$ avec
                leurs bases respectives $B$ et $B'$ ainsi qu'une application linéaire
                $u \in L(E, E')$ et $M_u$, $\forall v \in E, M_u \cdot \Psi_B(v)$
                nous donne les coefficients $\lambda'_i$ pour exprimer $u(v)$ dans la
                base $B'$. Pour refaire un vecteur $\in E'$, il faut appliquer $\Phi_{B'}$
                sur cet élément de $\mathbb K^n$.

                Donc $\forall v \in E, u(v) = \Phi_{B'}(M_u \cdot \Psi_B(v))$.
            \end{mdframed}

            \begin{mdframed}[linecolor=green]
                \textcolor{green}{\textbf{Exemple :}}

                Soit $u : \R^3 \to \R^2 : (x, y, z) \mapsto (3x-2y+z, x+y-z)$.
                Posons $B$, base canonique de $\R^2$ et $B'$ base canonique de $\R^3$. \\
                $\left.\begin{aligned}
                    u(e_1) &= u(1,0,0) = (3,1)\\
                    u(e_2) &= u(0,1,0) = (-2,1)\\
                    u(e_3) &= u(0,0,1) = (1,-1)
                \end{aligned}\right\}$
                $M_u = \begin{pmatrix} 3 & -2 & 1 \\  1 & 1 & -1 \end{pmatrix}$

                $M_u \cdot \begin{pmatrix} x \\ y \\ z \end{pmatrix} =
                \begin{pmatrix} 3x-2y+z \\ x+y-z \end{pmatrix}$.
            \end{mdframed}

            \begin{mdframed}
                \textcolor{blue}{Définition (rappel) :}

                Nous définissons $\Phi_{BB'} : \MatK(m, n) \to L(E, E') : [a_{ji}] \mapsto u$
                telle que $u(e_i) = \displaystyle \sum^n_{j=1} a_{j,i}e'_j \quad \forall i, 1 \leq i \leq n$,
                une application qui relie chaque matrice à son application linéaire.

                À nouveau, nous définissons également sa réciproque :
                $\Psi_{BB'} = \Phi_{BB'}^{-1} : L(E, E') \to \MatK(m, n) : u \mapsto [a_{ji}]$.
            \end{mdframed}

            \begin{mdframed}[linecolor=red]
                \textcolor{red}{\textbf{Théorème :}}

                $\Phi_{BB'}([a_{j,i}])$ est un isomorphisme.

                $M$ est inversibles $\Leftrightarrow u = \Phi_{BB'}(M)$ est un isomorphisme.
                Ou de façon équivalente, $u \in L(E,E')$ est bijective
                $\Leftrightarrow M = \Psi_{BB'}(u)$ est inversible. En quel cas
                $\Psi_{B'B}(u^{-1}) = M^{-1}$ ou encore $u^{-1} = \Phi_{B'B}(M)$.
            \end{mdframed}

        \subsection{Matrice de passage}
            \begin{mdframed}[linecolor=blue]
                \textcolor{blue}{\textbf{Définition :}}

                $P = \Psi_{BB'}(Id_E)$ est appelée la \textbf{matrice de passage} de $B$ à $B'$
                avec $Id_E : E \to E : v \mapsto v$.

                Autrement dit, la matrice associée à l'application qui envoie tout vecteur $v$ sur lui-même
                mais exprimé de la base $B$ vers la base $B'$ est appelée matrice de passage.
            \end{mdframed}

            \begin{mdframed}[linecolor=red]
                \textcolor{red}{\textbf{Théorème :}}

                Soient $u = Id_E$, $v \in E$ et deux bases de $E$, $B = \{e_1, \ldots, e_n\}, B' = \{e'_1, \ldots, e'_n\}$.
                $u(v) = \displaystyle \sum_{i=1}^n \lambda_i u(e_i) = \displaystyle \sum_{i=1}^n \lambda_i \sum_{j=1}^n a_{ji} e'_j$.
                Ou encore $u(v) = \displaystyle \sum_{j=1}^n\lambda'_j e'_j$ avec
                $\lambda'_j = \displaystyle \sum_{i=1}^n \lambda_i a_{ji}$.

                On a donc $\begin{pmatrix}\lambda'_1 \\ \vdots \\ \lambda'_n\end{pmatrix} = \Psi_{B'}(v) = \Psi_{BB'}(Id_E) \cdot \Psi_{B}(v)$.
            \end{mdframed}

            \begin{mdframed}[linecolor=red]
                \textcolor{red}{\textbf{Corrolaire :}}

                Si $P = \Psi_{BB'}(Id)$ est la matrice de passage de $B$ à $B'$,
                alors la matrice de passage de $B'$ à $B$ est
                $P^{-1} = \Psi_{B'B}(Id) = (\Psi_{BB'}(Id))^{-1}$.
            \end{mdframed}

            \begin{mdframed}[linecolor=green]
                \textcolor{green}{\textbf{Exemple :}}

                Soit $B_0 = \{e_1, e_2\}$, base canonique de $\R^2$ et $B' = \{(1,1), (-1,1)\} = \{e'_1, e'_2\}$.

                Commençons par déterminer ce que vaut $P$, la matrice de passage de la base $B_0$ à la base $B'$.

                \[
                    \left \{
                        \begin{aligned}
                        e_1=a_{1, 1}e'_1 +a_{2, 1}e'_2\\
                        e_2=a_{1, 2}e'_1 +a_{2, 2}e'_2
                        \end{aligned}
                    \right .
                    \Leftrightarrow
                    \left \{
                        \begin{aligned}
                            (1, 0) &= a_{1, 1}(1, 1) + a_{2, 1} (-1, 1) \Leftrightarrow
                            \left \{
                                \begin{aligned}
                                    a_{1,1} - a_{2,1} &= 1\\
                                    a_{1,1} + a_{2,1} &= 0
                                \end{aligned}
                            \right .
                            \Leftrightarrow
                            \left \{
                                \begin{aligned}
                                    a_{1, 1} &= \frac{1}{2}\\
                                    a_{2, 1} &= \frac{-1}{2}
                                \end{aligned}
                            \right .\\
                            (0, 1) &= a_{1, 2} (1, 1) + a_{2, 2} (-1, 1) \Leftrightarrow
                            \left \{
                                \begin{aligned}
                                    a_{1, 2} - a_{2, 2} &= 0\\
                                    a_{1, 2} + a_{2, 2} &= 1
                                \end{aligned}
                            \right .
                            \Leftrightarrow
                            \left \{
                                \begin{aligned}
                                    a_{1, 2} &= \frac{1}{2}\\
                                    a_{2, 2} &= \frac{1}{2}
                                \end{aligned}
                             \right .\\
                        \end{aligned}
                    \right .
                \]

                La matrice de passage, $M_{B_0B'} = \begin{pmatrix} \frac{1}{2} & \frac{1}{2} \\ -\frac{1}{2} & \frac{1}{2} \end{pmatrix}$

                Voyons ce que cela donne.

                Les coordonnées de $(x, y)$ dans la base $B'$ sont les suivantes :
                $\begin{pmatrix} \frac{1}{2} & \frac{1}{2} \\ -\frac{1}{2} & \frac{1}{2} \end{pmatrix} \cdot
                \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} \frac{x + y}{2} \\ \frac{-x + y}{2} \end{pmatrix}$.

                Autrement dit, tout point exprimé par les coordonnées $(x, y)$
                dans la base canonique sera exprimés $(\frac {x+y}2, \frac{y-x}2)$
                dans la base $B'$ choisie ici arbitrairement.

                Vérifions les calculs :

                $\frac{x + y}{2}e'_1 + \frac{-x + y}{2}e'_2 = \frac{x + y}{2}(1, 1) + \frac{-x + y}{2}(-1, 1) =
                (\frac{x + y}{2} - \frac{-x + y}{2}, \frac{x + y}{2} + \frac{-x + y}{2}) = (x, y)$

                En développant les calculs avec expression de la base, on voit bien qu'un point de coordonnée
                $(\frac {x+y}2, \frac{y-x}2)$ est identique à un point de coordonnée $(x, y)$ dans la base canonique.

                Essayons maintenant de déterminer ce que vaut la matrice de passage
                de la base $B'$ vers la base canonique $B_0$.

                $e'_1 = (1, 1) = (1, 0) + (0, 1) = e_1 + e_2 \\
                e'_2 = (-1, 1) = -1(1, 0) + (0, 1) = -e_1 + e_2 \\
                M_{B'B_0} = \begin{pmatrix}  1 & -1 \\ 1 & 1 \end{pmatrix}$

                Vérifions si $M_{B'B_0} \cdot M_{B_0B'} \stackrel{?}{=} I$ :

                $\begin{pmatrix} \frac{1}{2} & \frac{1}{2} \\ -\frac{1}{2} & \frac{1}{2} \end{pmatrix} \cdot
                \begin{pmatrix} 1 & -1 \\ 1 & 1 \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} = I$

                Nous avons bien le produit des matrices qui vaut l'identité, ce qui veut dire que
                les matrices sont bien associées à des applications réciproques.
                Et par calcul, nous avons montré que ces applications sont les applications unités.
            \end{mdframed}

            \begin{mdframed}[linecolor=red]
                \textcolor{red}{\textbf{Théorème :}}

                $u \in L(E,E), M = \Psi_{BB}(u), P = \Psi_{BB'}(Id_E),
                M' = \Psi_{B'B'}(u), P^{-1} = \Psi_{B'B}(Id_E)$

                $M' = P^{-1} \cdot M \cdot P \\
                (M')^n = P^{-1} \cdot M^n \cdot P$
            \end{mdframed}

        \subsection{Déterminant de matrice}
            \subsubsection{Calculs de déterminants}

                \begin{mdframed}[linecolor=blue]
                    \textcolor{blue}{\textbf{Définition :}}

                    Le \textbf{déterminant} d'une matrice $2\times2$ :
                    $M = \begin{pmatrix} a & b \\ c & d \end{pmatrix} = ad- bc$.

                    Le déterminant se note entre $|M|$ ou $\det(M)$.
                \end{mdframed}

                \begin{mdframed}[linecolor=cyan]
                    \textcolor{cyan}{\textbf{Remarque :}}

                    Seules les matrices carrées ont un déterminant (unique).
                \end{mdframed}

                \begin{mdframed}[linecolor=blue]
                    \textcolor{blue}{\textbf{Définition :}}

                    On peut calculer le déterminant d'une matrice
                    $3 \times 3$ en utilisant la règle de Sarrus.

                    Soit $M = \begin{pmatrix}
                        a_{1,1} & a_{1,2} & a_{1,3} \\
                        a_{2,1} & a_{2,2} & a_{2,3} \\
                        a_{3,1} & a_{3,2} & a_{3,3}
                    \end{pmatrix} \in \MatK(3, 3)$

                    %Ne veut rien dire, à corriger Lolo...
                    On va "recopier" en faisant une translation de la matrice
                    à sa droite tracé ses diagonales, d'abord du coin supérieur gauche,
                    en multipliant tout les termes. Puis on additionne avec les 2 diagonales à droite.
                    Puis on part du coin inférieur gauche, on multiplie les diagonales montantes
                    (en pointillés sur le schéma) et on soustraie avec les 2 diagonales à sa droite.\\
                    \begin{tikzpicture}[>=stealth]
                        \matrix [%
                          matrix of math nodes,
                          column sep=1em,
                          row sep=1em
                        ] (sarrus) {%
                          a_{1,1} & a_{1,2} & a_{1,3} & a_{1,1} & a_{1,2} \\
                          a_{2,1} & a_{2,2} & a_{2,3} & a_{2,1} & a_{2,2} \\
                          a_{3,1} & a_{3,2} & a_{3,3} & a_{3,1} & a_{3,2} \\
                        };

                        \path ($(sarrus-1-1.north west)-(0.5em,0)$) edge ($(sarrus-3-1.south west)-(0.5em,0)$)
                              ($(sarrus-1-3.north east)+(0.5em,0)$) edge ($(sarrus-3-3.south east)+(0.5em,0)$)
                              (sarrus-1-1)                          edge            (sarrus-2-2)
                              (sarrus-2-2)                          edge[->]        (sarrus-3-3)
                              (sarrus-1-2)                          edge            (sarrus-2-3)
                              (sarrus-2-3)                          edge[->]        (sarrus-3-4)
                              (sarrus-1-3)                          edge            (sarrus-2-4)
                              (sarrus-2-4)                          edge[->]        (sarrus-3-5)
                              (sarrus-3-1)                          edge[dashed]    (sarrus-2-2)
                              (sarrus-2-2)                          edge[->,dashed] (sarrus-1-3)
                              (sarrus-3-2)                          edge[dashed]    (sarrus-2-3)
                              (sarrus-2-3)                          edge[->,dashed] (sarrus-1-4)
                              (sarrus-3-3)                          edge[dashed]    (sarrus-2-4)
                              (sarrus-2-4)                          edge[->,dashed] (sarrus-1-5);

                        \foreach \c in {1,2,3} {\node[anchor=south] at (sarrus-1-\c.north) {$+$};};
                        \foreach \c in {1,2,3} {\node[anchor=north] at (sarrus-3-\c.south) {$-$};};
                    \end{tikzpicture} \\
                    % Source : http://tex.stackexchange.com/questions/32978/typesetting-a-matrix-with-crossing-arrows-on-it

                    $= a_{1,1} a_{2,2} a_{3,3} + a_{1,2} a_{2,3} a_{3,1} + a_{1,3} a_{2,1} a_{3,2} - a_{3,1} a_{2,2} a_{1,3} - a_{3,2} a_{2,3} a_{1,1} - a_{3,3} a_{2,1} a_{1,2} $\\

                D'autres manières existent pour calculer des déterminants de matrices $3 \times 3$.
                \end{mdframed}

        \subsubsection{Groupe de permutation}
            \begin{mdframed}[linecolor=blue]
                \textcolor{blue}{\textbf{Définition :}}

                % on va noter \delta pour les permutations et \mathfrak{S} pour les groupe
                Soit $\mathfrak{S}_n$, groupe des permutations de $n$ éléments, c'est-a-dire
                l'ensemble des bijections de $\{1, ..., n\}$ dans lui-même. Chaque permutation est notée $\delta$.\\
                $\#\mathfrak{S}_n = n!$ (factorielle)
            \end{mdframed}

            \begin{mdframed}[linecolor=green]
                \textcolor{green}{\textbf{Exemple :}}

                    \noindent $\mathfrak{S}_1 = \{(1)\} \\
                    \mathfrak{S}_2 = \{(1, 2), (2, 1)\}\\
                    \mathfrak{S}_3 = \{(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1)\}$
            \end{mdframed}

            \begin{mdframed}[linecolor=blue]
                \textcolor{blue}{\textbf{Définition :}}

                Les permutations les plus simples sont les transpositions, notées $\tau_{ij}$.
                Définies comme:

                \[
                    \left \{
                        \begin{aligned}
                            \tau_{ij}(i) &= j \\
                            \tau_{ij}(j) &= i \\
                            \tau_{ij}(k) &= k \quad \text{si } k \neq i \text{ et } k \neq j
                        \end{aligned}
                    \right .
                \]

                Ce qui revient à échanger 2 éléments de places (donc sans toucher aux autres).
            \end{mdframed}

            \begin{mdframed}[linecolor=green]
                \textcolor{green}{\textbf{Exemple :}}

                Avec (1, 2, 3) comme élément de départ :

                \begin{itemize}
                    \item $(1, 3, 2) = \tau_{2, 3}$ on a juste changé de position
                          l'élément en 2eme position avec celui en 3eme position ;
                    \item $(2, 3, 1) = \tau_{1, 2} \circ \tau_{2, 3}$ d'abord
                          on échange la position 1 avec le 2, ce qui donne
                          (2, 1, 3), puis on change celui de 2eme position avec
                          celui en 3eme ($\tau_{2, 3}$) ce qui donne (2, 3, 1). \\
                          Plus mathématiquement :
                          \[
                            \left .
                                \begin{aligned}
                                    (\tau_{1, 2} \circ \tau_{2, 3}) (1) &= \tau_{1, 2}(\tau_{2, 3}(1)) &= \tau_{1, 2}(1) &= 2\\
                                    (\tau_{1, 2} \circ \tau_{2, 3}) (2) &= \tau_{1, 2}(\tau_{2, 3}(2)) &= \tau_{1, 2}(3) &= 3\\
                                    (\tau_{1, 2} \circ \tau_{2, 3}) (3) &= \tau_{1, 2}(\tau_{2, 3}(3)) &= \tau_{1, 2}(2) &= 1
                                \end{aligned}
                            \right \}
                            \Rightarrow \tau_{1,2} \circ \tau_{2,3} = (2,3,1)
                          \]
                \end{itemize}
            \end{mdframed}

            \begin{mdframed}[linecolor=red]
                \textcolor{red}{\textbf{Théorème :}}

                $\forall \sigma \in \mathfrak{S}_n, \exists k$ transpositions $\tau_1, ..., \tau_k$ telles que $\sigma = \tau_k \circ \tau_{k-1} \circ ... \circ \tau_1$.
                De plus si on a $k'$ autres transpositions avec $\tau_k' \circ \tau_{k'-1} \circ ... \circ \tau_1 = \sigma$ alors $k \equiv k'[2]$ c'est à dire que $k$ et $k'$ sont tous les deux soit paires, soit impaires,
                ou encore $(-1)^k = (-1)^{k'}$.
            \end{mdframed}

            \begin{mdframed}[linecolor=green]
                \textcolor{green}{\textbf{Exemple :}}

                Pour passer de (1, 2, 3) à (1, 3, 2) :

                \begin{itemize}
                    \item $\tau_{2, 3} ($ devient $(1, 3, 2))$ ;
                    \item $\tau_{1, 2} ($ devient $(2, 1, 3)) \circ \tau_{1, 3} ($devient $(3, 1, 2)) \circ \tau_{1, 2} ($ devient $(1, 3, 2))$ ;
                    \item etc... quoi qu'il arrive on aura ici toujours un nombre impair de transpositions
                          à faire pour atteindre le résultat final.
                \end{itemize}
            \end{mdframed}

            \begin{mdframed}[linecolor=blue]
                \textcolor{blue}{\textbf{Définition :}}

                Le nombre $(-1)^k$ est noté $\sign(\sigma)$ et est appelé la
                \textbf{signature} de $\sigma$. On peut s'en servir pour calculer
                le déterminant d'une matrice $n \times n$.
            \end{mdframed}

        \subsection{Calcul général de déterminant de matrice}

            \begin{mdframed}[linecolor=blue]
                \textcolor{blue}{\textbf{Définition :}}

                Soit $M = \begin{pmatrix}
                    a_{1,1} & \cdots & a_{1,n} \\
                    \vdots  & \ddots & \vdots  \\
                    a_{n,1} & \cdots & a_{n,n}
                \end{pmatrix},
                \det(M) = \displaystyle \sum_{\sigma \in \mathfrak{S}_n} \cdot \sign(\sigma) \cdot \displaystyle \prod^n_{i=1}a_{\sigma(i), i}$
            \end{mdframed}

            \begin{mdframed}[linecolor=green]
                \textcolor{green}{\textbf{Exemple :}}

                Prenons la formule pour $n=3:\\
                \mathfrak{S}_3 = \{(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1)\}$

                \begin{itemize}
                    \item $(1,2,3) : (\sign = 1) \land (\prod = a_{1,1} \cdot a_{2,2} \cdot a_{3,3})$
                    \item $(1,3,2) : (\sign = -1) \land (\prod = a_{1,1} \cdot a_{3,2} \cdot a_{2,3})$
                    \item $(2,1,3) : (\sign = -1) \land (\prod = a_{2,1} \cdot a_{1,2} \cdot a_{3,3})$
                    \item $(2,3,1) : (\sign = 1) \land (\prod = a_{2,1} \cdot a_{3,2} \cdot a_{1,3})$
                    \item $(3,1,2) : (\sign = 1) \land (\prod = a_{3,1} \cdot a_{1,2} \cdot a_{2,3})$
                    \item $(3,2,1) : (\sign = -1) \land (\prod = a_{3,1} \cdot a_{2,2} \cdot a_{1,3})$
                \end{itemize}
                % repenser la structure ?
                Maintenant on utilise la formule complète en sommant les différents éléments :\\
                $\det(M) = a_{1, 1}a_{2, 2}a_{3, 3} - a_{1,}a_{3, 2}a_{2, 3} - a_{2, 1}a_{1, 2}a_{3, 3} + a_{2, 1}a_{3, 2}a_{1, 3} + a_{3, 1}a_{1, 2}a_{2, 3} - a_{3, 1}a_{2, 2}a_{1, 3} $,\\
                ce qui redonne la formule de Sarrus !
            \end{mdframed}

        \subsection{Propriétés du déterminant}
            \begin{mdframed}[linecolor=blue]
                \textcolor{blue}{\textbf{Propriétés algébriques :}}

                Soient $M$ et $N$, deux matrices de mêmes dimension $n \times x$ et $\lambda$ un scalaire.
                $M^t$ est la transposée de $M$ (inversion des lignes et colonnes).

                \begin{itemize}
                    \item $\det(M^t) = \det(M)$
                    \item $\det(M+N) \neq \det(M) + \det(N)$
                    \item $\det(\lambda M) = \lambda^n \cdot \det(M)$
                    \item $\det(M \cdot N) = \det(M) \cdot \det(N)$
                \end{itemize}

                De plus, le déterminant n'est pas linéaire (car la somme n'est pas respecté).
                En revanche il est multi-linéaire, impliquant :

                \begin{itemize}
                    \item $\det(c_1 | \cdots | \lambda \cdot c_i | \cdots | c_n) = \lambda \cdot \det(c_1 | \cdots | c_i | \cdots | c_n)$ ;
                    \item $\det(c_1 | \cdots |  c_i + c_j | \cdots | c_n) = \det(c_1 | \cdots | c_i | \cdots | c_n) + \det(c_1 | \cdots | c_j | \cdots | c_n)$ ;
                    \item Si deux colonnes de $M$ forment une famille liée
                          (2 colonnes identiques ou multiple l'une de l'autre ou encore
                          somme/soustraction avec une/des autres donne 0) , alors $\det(M) = 0$.
                \end{itemize}
            \end{mdframed}

            \begin{mdframed}[linecolor=cyan]
                \textcolor{cyan}{\textbf{Remarque :}}

                Comme le déterminant de la matrice est égal à celui de sa transposée,
                toutes les propriétés énoncées ci-dessus, mentionnant les colonnes,
                sont également valable pour les lignes.
            \end{mdframed}

            \bigskip

            \begin{mdframed}[linecolor=blue]
                \textcolor{blue}{\textbf{Définition :}}

                Soit une matrice $M \in Mat(m, n)$ avec $a_{j,i}$ l'élément de $M$
                en $j$eme ligne et $i$eme colonne.

                $M_{j,i} = \begin{pmatrix}
                    a_{1,1}   & \cdots & a_{1,i-1}   & a_{1,i+1}   & \cdots & a_{1,n}   \\
                    \vdots    &        &             &             &        & \vdots    \\
                    a_{j-1,1} & \cdots & a_{j-1,i-1} & a_{j-1,i+1} & \cdots & a_{j-1,n} \\
                    a_{j+1,1} & \cdots & a_{j+1,i-1} & a_{j+1,i+1} & \cdots & a_{j+1,n} \\
                    \vdots    &        &             &             &        & \vdots    \\
                    a_{m,1}   & \cdots & a_{m,i-1}   & a_{m,i+1}   & \cdots & a_{m,n}
                \end{pmatrix} \in \MatK(m-1, n-1)$

                On a donc enlevé la ligne et la colonne à l'emplacement $M_{j,i}$
                et rattaché ensemble les morceaux de matrices.
            \end{mdframed}

            \begin{mdframed}[linecolor=green]
                \textcolor{green}{\textbf{Exemple :}}

                $M = \begin{pmatrix}
                    a_{1,1} & a_{1,2} & a_{1,3} \\
                    a_{2,1} & a_{2,2} & a_{2,3} \\
                    a_{3,1} & a_{3,2} & a_{3,3}
                \end{pmatrix}$
                
                \begin{itemize}
                    \item $M_{1,1} =
                          \begin{pmatrix}
                            \MyTikzmark{leftA}{a_{1,1}} & a_{1,2} & \MyTikzmark{rightA}{a_{1,3}} \\
                            a_{2,1} & a_{2,2} & a_{2,3} \\
                            \MyTikzmark{bottomA}{a_{3,1}} & a_{3,2} & a_{3,3}
                          \end{pmatrix}
                          \DrawVLine[black, thick, opacity=0.5]{leftA}{bottomA}
                          \DrawHLine[black, thick, opacity=0.5]{leftA}{rightA}
                          =
                          \begin{pmatrix}
                            a_{2,2} & a_{2,3} \\
                            a_{3,2} & a_{3,3}
                          \end{pmatrix}$ ;
                    \item $M_{2,2} =
                          \begin{pmatrix}
                            a_{1,1} & \MyTikzmark{topB}{a_{1,2}} & a_{1,3}\\
                            \MyTikzmark{leftB}{a_{2,1}}  & a_{2,2} & \MyTikzmark{rightB}{a_{2,3}} \\
                            a_{3,1} & \MyTikzmark{bottomB}{a_{3,2}} & a_{3,3}
                          \end{pmatrix}
                          \DrawVLine[black,   thick, opacity=0.5]{topB}{bottomB}
                          \DrawHLine[black,   thick, opacity=0.5]{leftB}{rightB}
                          =
                          \begin{pmatrix}
                            a_{1,1} & a_{1,3} \\
                            a_{3,1} & a_{3,3}
                          \end{pmatrix}$ ;
                    \item $M_{2,3} =
                          \begin{pmatrix}
                            a_{1,1} & a_{1,2} & \MyTikzmark{topC}{a_{1,3}}\\
                            \MyTikzmark{leftC}{a_{2,1}}  & a_{2,2} & \MyTikzmark{rightC}{a_{2,3}} \\
                            a_{3,1} & a_{3,2} & \MyTikzmark{bottomC}{a_{3,3}}
                          \end{pmatrix}
                          \DrawVLine[black,   thick, opacity=0.5]{topC}{bottomC}
                          \DrawHLine[black,   thick, opacity=0.5]{leftC}{rightC}
                          =
                          \begin{pmatrix}
                            a_{1,1} & a_{1,2} \\
                            a_{3,1} & a_{3,2}
                          \end{pmatrix}$.
                \end{itemize}
            \end{mdframed}

            \begin{mdframed}[linecolor=red]
                \textcolor{red}{\textbf{Théorème :}}
                
                Soit $M$ une matrice carrée. $\det(M) = \displaystyle \sum^n_{j=1}a_{j,i} \cdot (-1)^{i+j}\cdot \underbrace{\det(M_{j,i})}_{\text{"mineur"}}\quad \forall 0 \leq i \leq n $ fixé
            \end{mdframed}

            \begin{mdframed}[linecolor=green]
                \textcolor{green}{\textbf{Exemple :}}\\

                $\begin{vmatrix}
                    -1 & -1 & 4 & 3  \\
                    2 & 0 & 1 & 0 \\
                    2 & 0 & 1 & 3 \\
                    1 & 1 & -2 & 2
                \end{vmatrix}$
                
                Choisissons la 2eme colonne ($i = 2$)  car elle contient le plus de zéros.
                On applique la formule (la somme) pour :
                
                \begin{itemize}
                    \item $j = 1 : a_{1, 2} \cdot (-1)^{1 + 2} \det(M_{1, 2}) = (-1) \cdot (-1) \cdot
                          \begin{vmatrix}
                            \MyTikzmark{leftD}{-1} & \MyTikzmark{topD}{-1} & 4 & \MyTikzmark{rightD}{3}  \\
                            2 & 0 & 1 & 0 \\
                            2 & 0 & 1 & 3 \\
                            1 & \MyTikzmark{botD}{1} & -2 & 2
                          \end{vmatrix}
                          \DrawVLine[black,   thick, opacity=0.5]{topD}{botD}
                          \DrawHLine[black,   thick, opacity=0.5]{leftD}{rightD}
                          = 1 \cdot
                          \begin{vmatrix}
                            2 & 1 & 0 \\
                            2 & 1 & 3 \\
                            1 & -2 & 2
                          \end{vmatrix} = 15$ ;
                    \item $j = 2 : a_{2, 2} \cdot (-1)^{2 + 2} \cdot \det(M_{2, 2}) = (0) \cdot (1) \cdot
                          \begin{vmatrix}
                            -1 & \MyTikzmark{topE}{-1} & 4 & 3  \\
                            \MyTikzmark{leftE}{2} & 0 & 1 & \MyTikzmark{rightE}{0} \\
                            2 & 0 & 1 & 3 \\
                            1 & \MyTikzmark{botE}{1} & -2 & 2
                          \end{vmatrix}
                          \DrawVLine[black,   thick, opacity=0.5]{topE}{botE}
                          \DrawHLine[black,   thick, opacity=0.5]{leftE}{rightE}
                          = 0 \cdot
                          \begin{vmatrix}
                            -1 & 4 & 3 \\
                            2 & 1 & 3 \\
                            1 & -2 & 2
                          \end{vmatrix} = 0$ ;
                    \item $j = 3 : a_{3, 2} \cdot (-1)^{3 + 2} \cdot \det(M_{3, 2}) = (0) \cdot (-1) \cdot
                          \begin{vmatrix}
                            -1 & \MyTikzmark{topF}{-1} & 4 & 3  \\
                            2 & 0 & 1 & 0 \\
                            \MyTikzmark{leftF}{2} & 0 & 1 & \MyTikzmark{rightF}{3} \\
                            1 & \MyTikzmark{botF}{1} & -2 & 2
                          \end{vmatrix}
                          \DrawVLine[black,   thick, opacity=0.5]{topF}{botF}
                          \DrawHLine[black,   thick, opacity=0.5]{leftF}{rightF}
                          = 0 \cdot
                          \begin{vmatrix}
                            -1 & 4 & 3 \\
                            2 & 1 & 0 \\
                            1 & -2 & 2
                          \end{vmatrix} = 0$ ;
                    \item $j = 4 : a_{4, 2} \cdot (-1)^{4 + 2} \cdot \det(M_{4, 2}) = (1) \cdot (1) \cdot
                          \begin{vmatrix}
                            -1 & \MyTikzmark{topG}{-1} & 4 & 3  \\
                            2 & 0 & 1 & 0 \\
                            2 & 0 & 1 & 3 \\
                            \MyTikzmark{leftG}{1} & \MyTikzmark{botG}{1} & -2 & \MyTikzmark{rightG}{2}
                          \end{vmatrix}
                          \DrawVLine[black,   thick, opacity=0.5]{topG}{botG}
                          \DrawHLine[black,   thick, opacity=0.5]{leftG}{rightG}
                          = 1 \cdot
                          \begin{vmatrix}
                            -1 & 4 & 3 \\
                            2 & 1 & 0 \\
                            2 & 1 & 3
                          \end{vmatrix} = -27$.
                \end{itemize}
                
                La somme de tous les résultats intermédiaires donne $\det(M) = 15 + 0 + 0 -27 = -12$.
            \end{mdframed}

            \begin{mdframed}[linecolor=cyan]
                \textcolor{cyan}{\textbf{Remarque :}}
                
                Afin de gagner du temps on peux aussi utiliser la 2eme propriété de
                multi-linéarité qui nous permet de changer les valeurs de notre colonne
                pour faire apparaitre plus de $0$, simplifiant ainsi les calculs.
            \end{mdframed}
            
            \newpage

            \begin{mdframed}[linecolor=green]
                \textcolor{green}{\textbf{Exemple :}}
                
                (En reprenant l'exemple ci-dessus)
                
                $\begin{vmatrix}
                    -1 & -1 & 4 & 3  \\
                    2 & 0 & 1 & 0 \\
                    2 & 0 & 1 & 3 \\
                    1 & 1 & -2 & 2
                \end{vmatrix}$
                
                On va additionner à la dernière ligne la première ligne: \\
                $\begin{vmatrix}
                    -1 & -1 & 4 & 3  \\
                    2 & 0 & 1 & 0 \\
                    2 & 0 & 1 & 3 \\
                    1-1 & 1-1 & -2+4 & 2+3
                \end{vmatrix} = \begin{vmatrix}
                    -1 & -1 & 4 & 3  \\
                    2 & 0 & 1 & 0 \\
                    2 & 0 & 1 & 3 \\
                    0 & 0 & 2 & 5
                \end{vmatrix}$
                
                On reprend la même colonne de départ (la seconde), et donc :
                
                $\begin{vmatrix}
                    \MyTikzmark{leftH}{-1} & \MyTikzmark{topH}{-1} & 4 & \MyTikzmark{rightH}{3}  \\
                    2 & 0 & 1 & 0 \\
                    2 & 0 & 1 & 3 \\
                    0 & \MyTikzmark{botH}{0} & 2 & 5
                \end{vmatrix}
                \DrawVLine[black,   thick, opacity=0.5]{topH}{botH}
                \DrawHLine[black,   thick, opacity=0.5]{leftH}{rightH}
                = 1 \cdot \begin{vmatrix}
                    2 & 1 & 0 \\
                    2 & 1 & 3 \\
                    0 & 2 & 5
                \end{vmatrix} = -12$
                
                Et on a pas besoin d'aller plus loin car tous les autres éléments
                de la colonne valent $0$.
            \end{mdframed}

            \bigskip

            \begin{mdframed}[linecolor=red]
                \textcolor{red}{\textbf{Théorème du rang:}}
                
                Soit $u : E \to E'$ avec $u$, une application linéaire, 
                $\dim(\Ker u) + \dim(\ImAppli u) = \dim(E)\\
                \Ker u = \{ v \in E\ |\ u(v)=0 \} \subset E \\
                \ImAppli u = \{ w \in E\ |\ \exists v \in E, u(v) = w \} = u(E)$

                \noindent $u$ est un isomorphisme $\Leftrightarrow \Ker u = \{0\} \Leftrightarrow \ImAppli u = E'$. \\
                Si $M$ est la matrice associée à $u$, $u$ est un isomorphisme
                $\Leftrightarrow M$ est inversible $\Leftrightarrow det(M) \neq 0$
            \end{mdframed}
\end{document}